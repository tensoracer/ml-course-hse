\documentclass[12pt,fleqn]{article}
\usepackage{../../vkCourseML}


\theorembodyfont{\rmfamily}
\newtheorem{esProblem}{Задача}
\setcounter{section}{1}

\begin{document}
\title{Машинное обучение\\Теоретическое домашнее задание №5}
\date{}
\author{}
\maketitle


\begin{esProblem}[3 балла]
    Известно, что решающие деревья плохо подходят для онлайн-обучения~---
    если приходят новые размеченные объекты, то нельзя быстро дообучить на них дерево,
    поскольку это может потребовать полного изменения структуры.
    Можно пересчитать прогнозы в листьях, но это вряд ли можно назвать полноценным дообучением.

    Как будет выглядеть дообучение модели~$k$ ближайших соседей,
    если поступили новые размеченные объекты?
    Сколько и каких операций для этого потребуется?
    Быстрее ли это, чем обучение модели с нуля на расширенной выборке?
    Для определённости считайте, что речь идёт о задаче многоклассовой классификации.
\end{esProblem}

\begin{esProblem}[7 баллов]
    Обучение метода одного ближайшего соседа заключается в запоминании обучающей выборки.
    На этапе применения модели к новому объекту~$x$ мы ищем ближайший к нему объект из обучения
    и возвращаем класс этого объекта.
    Воспользуемся несколько иным взглядом на применение такой модели.

    Допустим, мы решаем задачу бинарной классификации, а в качестве функции расстояния используем
    евклидову метрику.

    Диаграммой Вороного, соответствующей выборке~$X$, назовём такое разбиение пространства
    на области, что каждая область состоит из точек, для которых одна и та же точка из выборки~$X$ является ближайшей.
    Более формально, диаграмма Вороного для выборки~$X$ состоит из~$\ell$ областей~$R_1, \dots, R_\ell$,
    определяемых как
    \[
        R_i = \{x \in \RR^d \cond \rho(x, x_i) < \rho(x, x_j), j \neq i\}.
    \]

    \begin{enumerate}
        \item Покажите, что для указанной постановки разделяющую поверхность между классами
            можно описать через~$n$ групп гиперплоскостей для некоторого~$n$:
            \begin{equation}
            \label{eq:knnVoronoi}
                a(x)
                =
                \sum_{j = 1}^{n}
                \prod_{r = 1}^{n_r}
                    [\langle w_{jr}, x \rangle < t_{jr}],
            \end{equation}
            причём каждая из этих гиперплоскостей образует одну из сторон
            одной из ячеек диаграммы Вороного.
        \item Допустим, мы решили вместо обучающей выборки хранить гиперплоскости, образующие разделяющую поверхность,
            и на этапе применения использовать формулу~\eqref{eq:knnVoronoi}.
            Даст ли такой подход гарантированный выигрыш по памяти или по времени применения для одного объекта
            в худшем случае?
            Ответ обоснуйте.
    \end{enumerate}
\end{esProblem}

\begin{esProblem}[6 балла]
    На лекциях говорилось, что критерий информативности для набора объектов~$R$ вычисляется на основе того,
    насколько хорошо их целевые переменные предсказываются константой~(при оптимальном выборе этой константы):
    \[
        H(R)
        =
        \min_{c \in \YY}
        \frac{1}{|R|}
        \sum_{(x_i, y_i) \in R}
            L(y_i, c),
    \]
    где~$L(y, c)$~--- некоторая функция потерь.
    Соответственно, чтобы получить вид критерия при конкретной функции потерь, необходимо аналитически
    найти оптимальное значение константы и подставить его в формулу для~$H(R)$.

    Выведите критерии информативности для следующих функций потерь:
    \begin{enumerate}
        \item $L(y, c) = (y - c)^2$;
        \item $L(y, c) = \sum_{k = 1}^{K} (c_k - [y = k])^2$;
        \item $L(y, c) = -\sum_{k = 1}^{K} [y = k] \log c_k$.
    \end{enumerate}
    У вас должны получиться дисперсия, критерий Джини и энтропийный критерий соответственно.
\end{esProblem}

\begin{esProblem}[4 балла]
    Запишите оценку сложности построения одного решающего дерева в зависимости
    от размера обучающей выборки~$\ell$, числа признаков~$d$, максимальной глубины дерева~$D$.
    В качестве предикатов используются пороговые функции~$[x_j > t]$.
    При выборе предиката в каждой вершине перебираются все признаки,
    а в качестве порогов рассматриваются величины~$t$, равные значениям данного признака
    на объектах, попавших в текущую вершину.
    Считайте сложность вычисления критерия информативности константной.
\end{esProblem}

\begin{esProblem}[задача не оценивается]
    Ответьте на вопросы:
    \begin{enumerate}
        \item Что такое решающее дерево? Как по построенному дереву найти прогноз для объекта?
        \item Зачем в вершинах нужны предикаты? Какие типы предикатов вы знаете? Приведите примеры.
        \item Почему для любой выборки можно построить решающее дерево, имеющее нулевую ошибку на ней?
        \item Почему не рекомендуется строить небинарные деревья~(т.е. имеющие больше двух потомков у каждой вершины)?
        \item Как устроен жадный алгоритм построения дерева? Какие у него параметры?
        \item Зачем нужны критерии информативности?
        \item Как задается критерий ошибки классификации? Критерий Джини? Энтропийный критерий? Какой у них смысл?
        \item Как задается критерий информативности, основанный на среднеквадратичной ошибке, в задачах регрессии?
        \item Какие критерии останова вы знаете?
        \item Что такое стрижка дерева?
        \item Какие методы обработки пропущенных значений вы знаете?
        \item Как можно учитывать категориальные признаки в решающем дереве?
        \item Как можно свести задачу перебора всех разбиений категориального признака
            к задаче поиска оптимального разбиения для вещественного признака?
    \end{enumerate}
\end{esProblem}

\end{document}
