\documentclass[12pt,fleqn]{article}
\usepackage{vkCourseML}
\hypersetup{unicode=true}
%\usepackage[a4paper]{geometry}
\usepackage[hyphenbreaks]{breakurl}

\interfootnotelinepenalty=10000

\begin{document}
\title{Лекция 15\\EM-алгоритм}
\author{Е.\,А.\,Соколов\\ФКН ВШЭ}
\maketitle

Существуют задачи, в которых помимо параметров и известных данных имеются
скрытые переменные~--- некоторые величины, которые существенно упрощают модель,
но неизвестны.
Ниже мы рассмотрим смеси распределений, которые являются примером такой модели.
Общим методом обучения моделей со скрытыми переменными является EM-алгоритм,
о котором и пойдёт речь в этом лекции.
%Например, в методе главных компонент наблюдаемыми данными является исходная выборка,
%параметрами~--- главные компоненты, а в качестве скрытых переменных выступают
%разложения объектов по этим компонентам.

\section{Смеси распределений}
Говорят, что распределение~$p(x)$ является~\emph{смесью распределений},
если его плотность имеет вид
\begin{equation}
\label{eq:mixture}
    p(x)
    =
    \sum_{k = 1}^{K} \pi_k p_k(x),
    \qquad
    \sum_{k = 1}^{K} \pi_k = 1,
    \qquad
    \pi_k \geq 0,
\end{equation}
где~$p_k(x)$~--- распределения компонент смеси,
$\pi_k$~--- априорные вероятности компонент,
$K$~--- число компонент.
Будем считать, что распределения компонент смеси принадлежат
некоторому параметрическому семейству:~$p_k(x) = \phi(x \cond \theta_k)$.

Каждую компоненту распределения~$p_k(x)$ можно рассматривать как кластер,
а значение данной плотности на объекте~--- как вероятность принадлежности
данному кластеру.
Таким образом, с помощью смеси распределений можно описывать~\emph{мягкую кластеризацию},
в которой каждый объект относится к каждому из кластеров с некоторой вероятностью.

Рассмотрим следующий эксперимент:
сначала из дискретного распределения~$\{\pi_1, \dots, \pi_K\}$
выбирается номер~$k$, а затем из
распределения~$\phi(x \cond \theta_k)$ выбирается значение~$x$.
Покажем, что распределение переменной~$x$ будет представлять собой смесь
вида~\eqref{eq:mixture}.

Введем~\emph{скрытую переменную}~$z$, отвечающую за выбор компоненты смеси.
Пусть она представляет собой~$K$-мерный
бинарный случайный вектор, ровно одна компонента которого равна единице:
\[
    z \in \{0, 1\}^K,
    \qquad
    \sum_{k = 1}^{K} z_k = 1.
\]
Вероятность того, что единице будет равна~$k$-я компонента, равна~$\pi_k$:
\[
    p(z_k = 1) = \pi_k.
\]
Запишем распределение сразу всего вектора:
\[
    p(z) = \prod_{k = 1}^{K} \pi_k^{z_k}.
\]

Если номер компоненты смеси известен, то случайная величина~$x$
имеет распределение~$\phi(x \cond \theta_k)$:
\[
    p(x \cond z_k = 1)
    =
    \phi(x \cond \theta_k),
\]
или, что то же самое,
\[
    p(x \cond z)
    =
    \prod_{k = 1}^{K}
    \Bigl[
        \phi(x \cond \theta_k)
    \Bigr]^{z_k}.
\]

Запишем совместное распределение переменных~$x$ и~$z$:
\[
    p(x, z)
    =
    p(z) p(x \cond z)
    =
    \prod_{k = 1}^{K}
    \Bigl[
        \pi_k \phi(x \cond \theta_k)
    \Bigr]^{z_k}.
\]

Чтобы найти распределение переменной~$x$, нужно избавиться от скрытой переменной:
\[
    p(x)
    =
    \sum_{z} p(x, z).
\]
Суммирование здесь ведется по всем возможным значениям~$z$,
то есть по всем~\mbox{$K$-мерным} бинарным векторам с одной единицей:
\[
    p(x)
    =
    \sum_{z} p(x, z)
    =
    \sum_{k = 1}^{K}
    \pi_k \phi(x \cond \theta_k).
\]
Мы получили, что распределение переменной~$x$ в описанном эксперименте
представляет собой смесь~$K$ компонент.

\section{Модели со скрытыми переменными}
Рассмотрим вероятностную модель с наблюдаемыми переменными~$X$ и параметрами~$\Theta$,
для которой задано правдоподобие~$\log p(X \cond \Theta)$.
Предположим, что в модели также существуют~\emph{скрытые переменные}~$Z$,
описывающие ее внутреннее состояние.
Тогда правдоподобие~$\log p(X \cond \Theta)$ называется~\emph{неполным},
а правдоподобие~$\log p(X, Z \cond \Theta)$~--- \emph{полным}.
Они связаны соотношением
\[
    \log p(X \cond \Theta)
    =
    \log \Biggl\{
        \sum_{Z} p(X, Z \cond \Theta)
    \Biggr\}.
\]
Как правило, знание скрытых переменных существенно упрощает правдоподобие
и позволяет достаточно просто оценить параметры~$\Theta$.

Рассмотрим пример со смесями распределений.
В качестве наблюдаемых переменных здесь выступает выборка~$X = \{x_1, \dots, x_\ell\}$,
в качестве скрытых переменных~--- номера компонент,
из которых сгенерированы объекты~$Z = \{z_1, \dots, z_\ell\}$~(здесь
каждый из~$z_i$ является~$K$-мерным вектором),
в качестве параметров~--- априорные вероятности и параметры
компонент~$\Theta = (\pi_1, \dots, \pi_K, \theta_1, \dots, \theta_K)$.
Неполное правдоподобие имеет вид
\[
    \log p(X \cond \Theta)
    =
    \sum_{i = 1}^{\ell}
        \log \Biggl\{
            \sum_{k = 1}^{K} \pi_k p(x_i \cond \theta_k)
        \Biggr\}.
\]
Правдоподобие здесь имеет вид~<<логарифм суммы>>.
Если приравнять нулю его градиент, то получатся сложные
уравнения, не имеющие аналитического решения.
Данное правдоподобие сложно вычислять, оно не является вогнутым и имеет много
локальных экстремумов, поэтому применение итерационных методов
для его непосредственной максимизации приводит к медленной сходимости.

Рассмотрим теперь полное правдоподобие:
\[
    \log p(X, Z \cond \Theta)
    =
    \sum_{i = 1}^{\ell}
    \sum_{k = 1}^{K}
        z_{ik}
        \Bigl\{
            \log \pi_k
            +
            \log \phi(x_i \cond \theta_k)
        \Bigr\}.
\]
Оно имеет вид~<<сумма логарифмов>>,
и позволяет аналитически найти оценки максимального правдоподобия
на параметры~$\Theta$ при известных переменных~$X$ и~$Z$.
Проблема же заключается в том, что нам не известны скрытые переменные~$Z$,
поэтому их необходимо оценивать одновременно с параметрами,
что никак не легче максимизации неполного правдоподобия.
Решение данной проблемы предлагается в~\emph{EM-алгоритме}.

\section{EM-алгоритм}
EM-алгоритм решает задачу максимизации полного правоподобия
путем попеременной оптимизации по параметрам и по скрытым переменным.

Опишем сначала <<наивный>> способ оптимизации.
Зафиксируем некоторое начальное приближение для параметров~$\Theta^\text{old}$.
При известных наблюдаемых переменных~$X$ и параметрах~$\Theta^\text{old}$
мы можем оценить скрытые переменные, найдя их наиболее правдоподобные значения:
\[
    Z^*
    =
    \argmax_{Z} p(Z \cond X, \Theta^\text{old})
    =
    \argmax_{Z} p(X, Z \cond \Theta^\text{old}).
\]
Зная скрытые переменные, мы можем теперь найти следующее приближение для параметров:
\[
    \Theta^\text{new}
    =
    \argmax_{\Theta} p(X, Z^* \cond \Theta).
\]
Повторяя итерации до сходимости, мы получим некоторый итоговый вектор параметров~$\Theta^*$.
Данная процедура, однако, далека от идеальной~--- ниже мы предложим подход, который
приводит к более качественным результатам.

Гораздо лучшие результаты можно получить, воспользовавшись байесовским подходом.
Как и прежде, зафиксируем вектор параметров~$\Theta^\text{old}$,
но вместо точечной оценки вычислим апостериорное распределение на скрытых
переменных~$p(Z \cond X, \Theta^\text{old})$.
В этом заключается~\emph{E-шаг} EM-алгоритма.

Усредним логарифм полного правдоподобия по всем возможным значениям
скрытых переменных~$Z$ с весами, равными апостериорным вероятностям
этих переменных~$p(Z \cond X, \Theta^\text{old})$:
\[
    Q(\Theta, \Theta^\text{old})
    =
    \mathbb{E}_{Z \sim p(Z \cond X, \Theta^\text{old})}
        \log p(X, Z \cond \Theta)
    =
    \sum_{Z}
        p(Z \cond X, \Theta^\text{old})
        \log p(X, Z \cond \Theta).
\]
Формально говоря, мы нашли матожидание логарифма полного правдоподобия
по апостериорному распределению на скрытых переменных.
На \emph{M-шаге} новый вектор параметров находится как максимизатор данного матожидания:
\[
    \Theta^\text{new}
    =
    \argmax_{\Theta} Q(\Theta, \Theta^\text{old})
    =
    \argmax_{\Theta}
    \sum_{Z}
        p(Z \cond X, \Theta^\text{old})
        \log p(X, Z \cond \Theta).
\]
Далее мы рассмотрим условия сходимости описанного итерационного процесса
и увидим, что при достаточно общих условиях любая
из его сходящихся подпоследовательностей сойдется к стационарной точке
неполного правдоподобия.

\section{Дивергенция Кульбака-Лейблера}
Дивергенция Кульбака-Лейблера~--- это мера расстояния между двумя
вероятностными распределениями, которая определяется как
\[
    \KL{q}{p}
    =
    \int
        q(x)
        \log \frac{q(x)}{p(x)}
        dx.
\]
В случае с дискретными распределениями она принимает вид
\[
    \KL{q}{p}
    =
    \sum_{x}
        q(x)
        \log \frac{q(x)}{p(x)}.
\]

KL-дивергенция определена только в том случае, когда
из~$p(x) = 0$ следует~$q(x) = 0$.
При вычислении мы считаем, что~$0 \log 0 = 0$
и~$0 \log \frac{0}{0} = 0$.

\begin{vkProblem}
    Покажите, что KL-дивергенция неотрицательна.
\end{vkProblem}

\begin{esSolution}
    Нам понадобится неравенство Йенсена в интегральной форме:
    для любой вогнутой функции~$f(x)$ выполнено
    \[
        f\left(
            \int \alpha(x) y(x) dx
        \right)
        \geq
        \int \alpha(x) f(y(x)) dx,
        \quad
        \int \alpha(x) dx = 1,
        \quad
        \alpha(x) \geq 0.
    \]

    Пользуясь данным неравенством и вогнутостью логарифма, получаем:
    \[
        \KL{q}{p}
        =
        - \int
            q(x)
            \log \frac{p(x)}{q(x)}
            dx
        \geq
        - \log\left(
            \int
            q(x)
            \frac{p(x)}{q(x)}
            dx
        \right)
        =
        - \log\left(
            \int
            p(x)
            dx
        \right)
        =
        0.
    \]

    Можно доказать данное утверждение и без использования неравенства Йенсена,
    если в определении используется натуральный логарифм.
    Заметим, что при~$y > 0$ имеет место неравенство~$\ln y \leq y - 1$,
    которое обращается в равенство только при~$y = 1$.
    Тогда:
    \[
        \KL{q}{p}
        =
        - \int
            q(x)
            \log \frac{p(x)}{q(x)}
            dx
        \geq
        - \int
            q(x)
            \left(
                \frac{p(x)}{q(x)} - 1
            \right)
            dx
        =
        \int q(x) dx
        -
        \int p(x) dx
        =
        0.
    \]
\end{esSolution}

Неравенство Йенсена обращается в равенство тогда и только тогда,
когда~$y(x) = \text{const}$.
В нашем случае это означает, что~$\frac{p(x)}{q(x)} = \text{const}$.
Поскольку~$q$ и~$p$~--- вероятностные распределения, это возможно только
при их равенстве.
Мы получили важное свойство KL-дивергенции:
она обращается в нуль тогда и только тогда,
когда ее аргументы равны.

\begin{vkProblem}
    Пусть заданы выборка~$X^\ell$ и распределение на объектах~$p(x \cond \theta)$,
    параметр которого мы хотим настроить под данную выборку.
    Эмпирическим распределением называется дискретное распределение на объектах,
    присваивающее каждому объекту из обучающей выборки вероятность~$1/\ell$:
    \[
        \hat p(x \cond X^\ell)
        =
        \sum_{i = 1}^{\ell}
            \frac{1}{\ell} [x = x_i].
    \]
    Покажите, что максимизация правдоподобия эквивалентна
    минимизации дивергенции Кульбака-Лейблера между эмпирическим
    распределением и
    модельным распределением:~$\KL{\hat p(x \cond X^\ell)}{p(x \cond \theta)}$.
\end{vkProblem}

\begin{esSolution}
    Распишем указанную дивергенцию:
    \begin{align*}
        \KL{
            \hat p(x \cond X^\ell)
        }{
            p(x \cond \theta)
        }
        &=
        \sum_{i = 1}^{\ell}
            \frac{1}{\ell}
            \log \frac{
                1/\ell
            }{
                p(x_i \cond \theta)
            }
        =\\
        &=
        \sum_{i = 1}^{\ell}
            \frac{1}{\ell}
            \log \frac{1}{\ell}
        -
        \frac{1}{\ell}
        \sum_{i = 1}^{\ell}
            \log p(x_i \cond \theta)
        \to \min_\theta.
    \end{align*}
    Отбросим константные члены:
    \[
        \sum_{i = 1}^{\ell}
            \log p(x_i \cond \theta)
        \to \max_\theta.
    \]
    Мы получили задачу максимизации логарифма правдоподобия.
\end{esSolution}

Таким образом, метод максимума правдоподобия старается
подобрать такие параметры модели, чтобы она давала равномерное
распределение на объектах выборки и присваивала нулевую вероятность
всем остальным объектам.

\section{Обоснование EM-алгоритма}
Представим неполное правдоподобие в виде суммы двух функций:
\begin{equation}
\label{eq:emDecomp}
    \log p(X \cond \Theta)
    =
    \LL(q, \Theta)
    +
    \KL{q}{p},
\end{equation}
где
\begin{align*}
    &\LL(q, \Theta)
    =
    \sum_Z
        q(Z)
        \log \frac{
            p(X, Z \cond \Theta)
        }{
            q(Z)
        },\\
    &\KL{q}{p}
    =
    -\sum_Z
        q(Z)
        \log \frac{
            p(Z \cond X, \Theta)
        }{
            q(Z)
        }.
\end{align*}
Здесь~$q(Z)$~--- это произвольное распределение на скрытых переменных.

\begin{vkProblem}
    Докажите, что это представление корректно.
\end{vkProblem}

\begin{esSolution}
    \begin{align*}
        \sum_Z
            &q(Z)
            \log \frac{
                p(X, Z \cond \Theta)
            }{
                q(Z)
            }
        -
        \sum_Z
            q(Z)
            \log \frac{
                p(Z \cond X, \Theta)
            }{
                q(Z)
            }
        =\\
        &=
        \sum_Z
            q(Z)
            \log \frac{
                p(X, Z \cond \Theta)
            }{
                p(Z \cond X, \Theta)
            }
        =\\
        &=
        \sum_Z
            q(Z)
            \log p(X \cond \Theta)
        =\\
        &=
        \log p(X \cond \Theta)
        \sum_Z
            q(Z)
        =\\
        &=
        \log p(X \cond \Theta).
    \end{align*}
\end{esSolution}

Заметим, что~$\LL(q, \Theta)$~--- это нижняя оценка на логарифм правдоподобия:
\[
    \log p(X \cond \Theta)
    =
    \LL(q, \Theta)
    +
    \underbrace{\KL{q}{p}}_{\geq 0}
    \geq
    \LL(q, \Theta).
\]
Чем~<<правильнее>> выбрано распределение~$q(Z)$, тем точнее эта оценка.
Будем по очереди максимизировать нижнюю оценку~$\LL(q, \Theta)$ по~$q$ и по~$\Theta$.
Зафиксируем сначала вектор параметров~$\Theta^\text{old}$
и найдем максимум по~$q$.
Заметим, что в разложении~\eqref{eq:emDecomp} левая часть
не зависит от~$q$, поэтому нижняя оценка будет максимальна тогда,
когда KL-дивергенция будет минимальна.
Мы знаем, что минимум дивергенции равен нулю и достигается
на равных распределениях.
Таким образом, нижняя оценка достигнет своего максимума
на~$q = p(Z \cond X, \Theta^\text{old})$.
Мы получили E-шаг EM-алгоритма~--- вычисление апостериорного распределения
на скрытых переменных.

Зафиксируем теперь~$q$ и найдем максимум нижней оценки по~$\Theta$.
Преобразуем задачу:
\begin{align*}
    \LL(q, \Theta)
    &=
    \sum_Z
        q(Z)
        \log \frac{
            p(X, Z \cond \Theta)
        }{
            q(Z)
        }
    =\\
    &=
    \sum_Z
        q(Z)
        \log p(X, Z \cond \Theta)
    -
    \sum_Z
        q(Z)
        \log q(Z)
    =\\
    &=
    \sum_Z
        p(Z \cond X, \Theta^\text{old})
        \log p(X, Z \cond \Theta)
    +
    \text{const}(\Theta)
    =\\
    &=
    Q(\Theta, \Theta^\text{old})
    +
    \text{const}(\Theta)
    \to
    \max_\Theta.
\end{align*}
Мы получили оптимизационную задачу с M-шага EM-алгоритма.

Описанный способ вывода E- и M-шагов позволяет получить
важное свойство EM-алгоритма~---~\emph{на каждой его итерации
значение правдоподобия не уменьшается}.
Действительно, после E-шага значение нижней оценки совпадает
со значением правдоподобия, а значит, максимизация
оценки на M-шаге приведет и к максимизации правдоподобия:
\[
    \log p(X \cond \Theta^\text{new})
    =
    \LL(q, \Theta^\text{new})
    +
    \KL{q}{p}
    \geq
    \LL(q, \Theta^\text{new})
    \geq
    \LL(q, \Theta^\text{old})
    =
    \log p(X \cond \Theta^\text{old}).
\]
Если правдоподобие ограничено сверху,
то последовательность значений правдоподобия~$\{p(X \cond \Theta^i)\}_i$
обязательно сойдется.
Здесь мы обозначили последовательность параметров, генерируемую EM-алгоритмом,
через~$\{\Theta^i\}_i$.

Существуют и более сильные утверждения о сходимости.
\begin{vkTheorem}[\cite{wu83convergence}]
    Пусть~$Q(\Theta, \Theta^\text{old})$ непрерывна по~$\Theta$ и~$\Theta^\text{old}$.
    Тогда все предельные точки последовательности~$\{\Theta^i\}_i$
    являются стационарными точками неполного правдоподобия~$p(X \cond \Theta)$,
    а последовательность~$\{p(X \cond \Theta^i)\}_i$ монотонно сходится
    к значению правдоподобия~$L^* = p(X \cond \Theta^*)$
    в одной из стационарных точек~$\Theta^*$.
\end{vkTheorem}
Обратим внимание на тот факт, что сходимость последовательности~$\{\Theta^i\}_i$
не гарантируется~--- у нее может быть несколько подпоследовательностей,
каждая из которых будет сходиться к своей стационарной точке.
Также отметим, что речь идет только о сходимости к~\emph{стационарной} точке;
сходимость к локальному максимуму гарантируется лишь для
некоторых семейств распределений~(например, для экспоненциальных~\cite{wu83convergence}).

Покажем одно из свойств EM-алгоритма.
\begin{vkProblem}
    Докажите, что если~$\Theta^i$ не является стационарной точкой
    логарифма правдоподобия, то следующее приближение~$\Theta^{i + 1}$,
    выданное EM-алгоритмом, будет отличаться от~$\Theta^i$.
\end{vkProblem}

\begin{esSolution}
    Пусть~$\Theta^i$ не является стационарной точкой,
    то есть
    \[
        \left.
        \nabla_\Theta
        \log p(X \cond \Theta) \right|_{\Theta^i}
        \neq 0.
    \]
    Выполним E-шаг, найдем апостериорное распределение~$q(\Theta^i)$,
    и запишем разложение правдоподобия:
    \[
        \log p(X \cond \Theta^i)
        =
        \LL(q, \Theta^i)
        +
        \underbrace{\KL{q(\Theta^i)}{p}}_{=0}.
    \]
    KL-дивергенция здесь равна нулю в силу выбора распределения~$q(\Theta^i)$.
    Поскольку на данном распределении достигается минимум дивергенции,
    ее градиент равен нулю:
    \[
        \left.
        \nabla_\Theta \KL{q(\Theta)}{p} \right|_{\Theta^i}
        =
        0.
    \]
    Получаем:
    \[
        \left.
        \nabla_\Theta
            \LL(q, \Theta) \right|_{\Theta^i}
        =
        \left.
        \nabla_\Theta
            \log p(X \cond \Theta) \right|_{\Theta^i}
        -
        \underbrace{\left.
        \nabla_\Theta
            \KL{q(\Theta)}{p} \right|_{\Theta^i}}_{=0}
        =
        \left.
        \nabla_\Theta
            \log p(X \cond \Theta) \right|_{\Theta^i}
        \neq
        0.
    \]
    Таким образом, точка~$\Theta^i$
    не является максимумом нижней оценки,
    и поэтому на M-шаге будет сделан переход к новой точке~$\Theta^{i + 1} \neq \Theta^i$.
\end{esSolution}

\begin{thebibliography}{1}
\bibitem{wu83convergence}
    \emph{Wu, C. F. Jeff} (1983).
    On the Convergence Properties of the EM Algorithm.~//
    Annals of Statistics, 11(1), p. 95-103.
\end{thebibliography}

\end{document}
