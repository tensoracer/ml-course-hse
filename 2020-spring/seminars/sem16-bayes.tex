\documentclass[12pt,fleqn]{article}
\usepackage{../lecture-notes/vkCourseML}
\hypersetup{unicode=true}
\usepackage{xcolor}
%\usepackage[a4paper]{geometry}
\usepackage{cases}

\interfootnotelinepenalty=10000
\title{Машинное обучение, ФКН ВШЭ\\Семинар №16}
\author{}
\date{}
\begin{document}
	\maketitle

\section{Байесовские методы машинного обучения}
Пусть~$X = \{x_1, \dots, x_\ell\}$~--- выборка,
$\XX$~--- множество всех возможных объектов,
$Y$~--- множество ответов.
В байесовском подходе предполагается, что обучающие
объекты и ответы на них~$(x_1, y_1), \dots, (x_\ell, y_\ell)$ независимо выбираются
из некоторого распределения~$p(x, y)$, заданного на множестве~$\XX \times Y$.
Данное распределение можно переписать как
\[
    p(x, y)
    =
    p(y) p(x \cond y),
\]
где~$p(y)$ определяет вероятности появления каждого из возможных ответов
и называется~\emph{априорным распределением},
а~$p(x \cond y)$ задает распределение объектов при фиксированном ответе~$y$
и называется~\emph{функцией правдоподобия}.

Если известны априорное распределение и функция правдоподобия,
то по формуле Байеса можно записать~\emph{апостериорное распределение}
на множестве ответов:
\[
    p(y \cond x)
    =
    \frac{
        p(x \cond y) p(y)
    }{
        \int_s p(x \cond s) p(s) ds
    }
    =
    \frac{
        p(x \cond y) p(y)
    }{
        p(x)
    },
\]
где знаменатель не зависит от~$y$ и является нормировочной константой.

\subsection{Оптимальные байесовские правила}
Пусть на множестве всех пар ответов~$Y \times Y$ задана функция
потерь~$L(y, s)$.
Наиболее распространенным примером для задач классификации
является ошибка классификации~$L(y, s) = [y \neq s]$,
для задач регрессии~--- квадратичная функция потерь~$L(y, x) = (y - s)^2$.
\emph{Функционалом среднего риска} называется матожидание функции потерь
по всем парам~$(x, y)$ при использовании алгоритма~$a(x)$:
\[
    R(a) = \EE L(y, a(x))
    =
    \int_{Y} \int_{\XX} L(y, a(x)) p(x, y) dx dy.
\]
Если распределение~$p(x, y)$ известно, то можно найти алгоритм~$a_*(x)$,
оптимальный с точки зрения функционала среднего риска.

\subsubsection{Классификация}
Начнем с задачи классификации с множеством ответом~$Y = \{1, \dots, K\}$
и функции потерь~$L(y, s) = [y \neq s]$.
Покажем, что минимум функционала среднего риска достигается
на алгоритме
\[
    a_*(x) = \argmax_{y \in Y} p(y \cond x).
\]

Для произвольного классификатора~$a(x)$ выполнена
следующая цепочка неравенств:
\begin{align*}
    R(a)
    &=
    \int_{Y} \int_{\XX} L(y, a(x)) p(x, y) dx dy
    =
    \\
    &=
    \sum_{y = 1}^{K} \int_{\XX} [y \neq a(x)] p(x, y) dx
    =
    \\
    &=
    \int_{\XX} \sum_{y \neq a(x)} p(x, y) dx
    =
    \left\{
    \int_{\XX} \sum_{y \neq a(x)} p(x, y) dx
    +
    \int_{\XX} p(x, a(x)) dx
    =
    1
    \right\}
    =
    \\
    &=
    1 - \int_{\XX} p(x, a(x)) dx
    \geq\\
    &\geq
    1 - \int_{\XX} \max_{s \in Y} p(x, s) dx
    =
    \\
    &=
    1 - \int_{\XX} p(x, a_*(x)) dx
    =
    \\
    &=
    R(a_*)
\end{align*}
Таким образом, средний риск любого классификатора~$a(x)$
не превосходит средний риск нашего классификатора~$a_*(x)$.

Мы получили, что оптимальный байесовский классификатор
выбирает тот класс, который имеет наибольшую апостериорную вероятность.
Такой классификатор называется~\emph{MAP-классификатором} (maximum a posteriori).

\subsubsection{Регрессия}

Напомним, что при выводе разложения на шум, смещение и разброс функционала среднего риска для задачи регрессии и функции потерь~$L(y, x) = (y - s)^2$ нами уже была получена формула оптимального алгоритма с точки зрения данного функционала:
\[
    a_*(x) = \EE (y \cond x)
    =
    \int_Y y p(y \cond x) dy.
\]
Иными словами, мы должны провести <<взвешенное голосование>>
по всем возможным ответам, причем вес ответа равен его
апостериорной вероятности.

\subsection{Байесовский вывод}
Основной проблемой оптимальных байесовских алгоритмов,
о которых шла речь в предыдущем разделе, является
невозможность их построения на практике, поскольку нам никогда
неизвестно распределение~$p(x, y)$.
Данное распределение можно попробовать восстановить по обучающей выборке,
при этом существует два подхода~--- параметрический и непараметрический.
Сейчас мы сосредоточимся на параметрическом подходе.

Допустим, распределение на парах~<<объект-ответ>> зависит от
некоторого параметра~$\theta$: $p(x, y \cond \theta)$.
Тогда получаем следующую формулу для апостериорной вероятности:
\[
    p(y \cond x, \theta)
    \propto
    p(x \cond y, \theta) p(y),
\]
где выражение~<<$a \propto b$>> означает~<<$a$ пропорционально~$b$>>.
Для оценивания параметров применяется~\emph{метод максимального правдоподобия}:
\[
    \theta_*
    =
    \argmax_\theta
        L(\theta)
    =
    \argmax_\theta
        \prod_{i = 1}^{\ell} p(x_i \cond y_i, \theta),
\]
где $L(\theta)$~--- функция правдоподобия.
Примером такого подхода может служить~\emph{нормальный дискриминантный анализ},
где предполагается, что функции правдоподобия являются нормальными распределениями
с неизвестными параметрами~$\theta = (\mu, \Sigma)$.

Иногда удобнее сразу задавать апостериорное распределение~---
например, в случае с линейной регрессией.
Будем считать, что задан некоторый вектор весов~$w$,
и метка объекта~$y(x)$ генерируется следующим образом:
вычисляется линейная функция~$\langle w, x \rangle$,
и к результату прибавляется нормальный шум:
\[
    y(x) = \langle w, x \rangle + \eps,
    \quad
    \eps \sim \mathcal{N}(0, \sigma^2).
\]
В этом случае апостериорное распределение примет вид
\begin{equation}
\label{eq:linRegProbModel}
    p(y \cond x, w)
    =
    \mathcal{N}(\langle w, x \rangle, \sigma^2).
\end{equation}

\begin{vkProblem}
    Покажите, что метод максимального правдоподобия для
    модели~\eqref{eq:linRegProbModel} эквивалентен
    методу наименьших квадратов.
\end{vkProblem}

\begin{esSolution}
    Запишем правдоподобие для выборки~$x_1, \dots, x_\ell$:
    \[
        L(w)
        =
        \prod_{i = 1}^{\ell} p(y_i \cond x_i, w)
        =
        \prod_{i = 1}^{\ell}
            \frac{1}{\sqrt{2 \pi \sigma^2}}
            \exp\left(
                - \frac{(y_i - \langle w, x_i \rangle)^2}{2 \sigma^2}
            \right).
    \]
    Перейдем к логарифму правдоподобия:
    \[
        \log L(w)
        =
        -\ell \log \sqrt{2 \pi \sigma^2} -
        \frac{1}{2 \sigma^2} \sum_{i = 1}^{\ell} (y_i - \langle w, x_i \rangle)^2
        \to \max_w.
    \]
    Убирая все члены, не зависящие от вектора весов~$w$,
    получаем задачу наименьших квадратов
    \[
        \sum_{i = 1}^{\ell} (y_i - \langle w, x_i \rangle)^2 \to \min_w.
    \]
\end{esSolution}

\paragraph{Байесовский вывод параметров.}
В некоторых случаях применение метода максимального правдоподобия
для поиска параметров приводит к плохим результатам.
Например, если имеет место мультиколлинеарность,
то функция правдоподобия имеет много минимумов,
и решение может оказаться переобученным.
Одним из подходов к устранению этой проблемы
является введение априорного распределения~\emph{на параметрах}.

Пусть~$p(\theta)$~--- априорное распределение на векторе параметров~$\theta$.
В качестве функции правдоподобия для данного вектора возьмем
апостериорное распределение на ответах~$p( y \cond x, \theta)$.
Тогда по формуле Байеса
\[
    p(\theta \cond y, x)
    =
    \frac{
        p(y \cond x, \theta) p(\theta)
    }{
        p(y \cond x)
    }.
\]

Вернемся к примеру с линейной регрессией.
Введем априорное распределение на векторе весов:
\[
    p(w_j)
    =
    \mathcal{N}(0, \alpha^2),
    \quad j = 1, \dots, d.
\]
Иными словами, мы предполагаем, что веса концентрируются вокруг нуля.

\begin{vkProblem}
    Покажите, что максимизация апостериорной вероятности~$p(w \cond y, x)$
    для модели линейной регрессии с нормальным априорным распределением
    эквивалентна решению задачи гребневой регрессии.
\end{vkProblem}

\begin{esSolution}
    Запишем апостериорную вероятность вектора весов~$w$ для выборки~$x_1, \dots, x_\ell$:
    \begin{align*}
        p(w \cond y, x)
        &=
        \prod_{i = 1}^{\ell}
            p(y_i \cond x_i, w) p(w)
        =\\
        &=
        \prod_{i = 1}^{\ell}
            \frac{1}{\sqrt{2 \pi \sigma^2}}
            \exp\left(
                - \frac{(y_i - \langle w, x_i \rangle)^2}{2 \sigma^2}
            \right)
            \prod_{j = 1}^{d}
                \frac{1}{\sqrt{2 \pi \alpha^2}}
                \exp\left(
                    - \frac{w_j^2}{2 \alpha^2}
                \right).
    \end{align*}
    Перейдем к логарифму и избавимся от константных членов:
    \[
        \log p(w \cond y, x)
        =
        -\frac{1}{2 \sigma^2} \sum_{i = 1}^{\ell} (y_i - \langle w, x_i \rangle)^2
        -\frac{\ell}{2 \alpha^2} \underbrace{\sum_{j = 1}^{d} w_j^2}_{= \|w\|^2}.
    \]
    В итоге получаем задачу гребневой регрессии
    \[
        \sum_{i = 1}^{\ell} (y_i - \langle w, x_i \rangle)^2  + \lambda \|w\|^2 \to \min_w,
    \]
    где~$\lambda = \frac{\ell}{2 \alpha^2}$.
\end{esSolution}

После того, как оптимальный вектор весов~$w_*$ найден,
мы можем найти распределение на ответах для нового объекта~$x$:
\[
    p(y \cond x, X, w_*)
    =
    \mathcal{N} (\langle x, w_* \rangle, \sigma^2).
\]
Выше мы выяснили, что оптимальным ответом будет
матожидание~$\EE (y \cond x) = \int y p(y \cond x, X, w_*) dy$.

С точки зрения байесовского подхода~\cite{murphy12probabilistic}
правильнее не искать моду~\footnote{
    Мода~--- точка максимума плотности.
} $w_*$ апостериорного распределения на параметрах и брать соответствующую
ей модель~$p(y \cond x, X, w_*)$,
а устроить~<<взвешенное голосование>> всех возможных моделей:
\[
    p(y \cond x, X)
    =
    \int p(y \cond x, w) p(w \cond Y, X) dw,
\]
где~$X = \{x_1, \dots, x_\ell\}$, $Y = \{y_1, \dots, y_\ell\}$.


%\subsection{Особенности байесовских алгоритмов}
%Основной проблемой оптимальных байесовских алгоритмов,
%о которых шла речь в предыдущем разделе, является
%невозможность их построения на практике, поскольку нам никогда
%неизвестно распределение~$p(x, y)$.
%Данное распределение можно попробовать восстановить по обучающей выборке,
%при этом существует два подхода~--- параметрический и непараметрический.
%Сейчас мы сосредоточимся на параметрическом подходе.

%Допустим, распределение на парах~<<объект-ответ>> зависит от
%некоторого параметра~$\theta$: $p(x, y \cond \theta)$.
%Тогда получаем следующую формулу для апостериорной вероятности:
%\[
%    p(y \cond x, \theta)
%    \propto
%    p(x \cond y, \theta) p(y),
%\]
%где выражение~<<$a \propto b$>> означает~<<$a$ пропорционально~$b$>>.
%Для оценивания параметров применяется~\emph{метод максимального правдоподобия}:
%\[
%    \theta_*
%    =
%    \argmax_\theta
%        L(\theta)
%    =
%    \argmax_\theta
%        \prod_{i = 1}^{\ell} p(x_i \cond y_i, \theta),
%\]
%где $L(\theta)$~--- функция правдоподобия.
%Примером такого подхода может служить~\emph{нормальный дискриминантный анализ},
%где предполагается, что функции правдоподобия являются нормальными распределениями:
%\begin{align*}
%    &a(x) = \argmax_{y \in Y} p(y) p(x \cond y),\\
%    &p(x \cond y) = \NN(x \cond \mu_y, \Sigma_y).
%\end{align*}
%Параметрами алгоритма являются средние~$\mu_y$ и
%ковариационные матрицы классов~$\Sigma_y$,
%которые оцениваются по выборке методом максимального
%правдоподобия.

%    \par Если предположить, что ковариационные матрицы классов равны,
%и оценивать их по всей выборке, то мы получим алгоритм,
%называемый~\emph{линейным дискриминантом Фишера}.
%Можно показать, что он является линейным:
%\[
%    a(x)
%    =
%    \argmax_{y \in Y} ( \langle w_y, x \rangle + w_{0y} ),
%\]
%причем~$w_y = \Sigma^{-1} \mu_y$.
%В случае двух классов~($Y = \{-1, +1\}$) классификатор принимает вид
%\begin{equation}
%\label{eq:ldaClassifier}
%    a(x)
%    =
%    \sign \left(
%        \langle w, x \rangle + b
%    \right)
%    \quad
%    w = \Sigma^{-1} (\mu_2 - \mu_1).
%\end{equation}

\subsection{Наивный байесовский классификатор}
\par Как было сказано ранее, при применении байесовского классификатора необходимо решить задачу восстановления плотности $p_y(x)$ для каждого класса $y \in \mathbb{Y}.$ Данная задача является довольно трудоёмкой и не всегда может быть решена, особенно в случае большого количества признаков, — в частности, если объектами являются тексты, приходится работать с крайне большим числом признаков, и восстановление плотности многомерного распределения не представляется возможным.
\par Для разрешения этой проблемы сделаем предположение о независимости признаков. В этом случае функция правдоподобия класса $y$ для объекта $x = \left( x_1, \dots, x_d\right)$ может быть представлена в следующем виде:
\begin{align*}
	p(x \cond y) = \prod_{j=1}^d p(x_j \cond y), 
\end{align*}
где $p(x_j \cond y)$ — одномерная плотность распределения $j$-ого признака объектов класса~$y \in Y.$ В этом случае формула байесовского решающего правила примет следующий вид:
\begin{align*}
	a(x) = \arg \max_{y \in Y} p(y \cond x) =
\arg \max_{y \in \mathbb{Y}} \left( \ln p(y) + \sum_{j=1}^d \ln p(x_j \cond y) \right).
\end{align*}

Предположение о независимости признаков существенно облегчает задачу, поскольку вместо решения задачи восстановления $d$-мерной плотности необходимо решить $d$ задач восстановления одномерных плотностей. Полученный классификатор называется \emph{наивным байесовским классификатором}.
\par Плотности отдельных признаков могут быть восстановлены различными способами (параметрическими и непараметрическими). Среди параметрических способов чаще всего используются нормальное распределение (для вещественных признаков), распределение Бернулли и мультиномиальное распределение (для дискретных признаков), благодаря которым получаются различные применяющиеся на практике модели.

\end{document} 
