\documentclass[12pt,fleqn]{article}

\usepackage{vkCourseML}

\usepackage{lipsum}
\usepackage{indentfirst}
\title{Машинное обучение, ФКН ВШЭ\\ SVM и логистическая регрессия}
\author{}
\date{}
\theorembodyfont{\rmfamily}
\newtheorem{esProblem}{Задача}
\begin{document}

\maketitle

\begin{esProblem}
    Роберта утверждает, что открыла новую дифференциируемую верхнюю границу для пороговой функции потерь,
    
    \[\tilde{L}(M_i) = \frac{9}{10} - \frac{1}{\pi} \cdot \arctan(M_i),\]
    где $M_i = y_i \cdot \langle w, x_i \rangle$. Права ли Роберта?
\end{esProblem}

\begin{esProblem}
    Позволяет ли предсказывать корректные вероятности экспоненциальная функция потерь~$L(y, z) = \exp(-yz)$?
\end{esProblem}

\begin{esProblem}
    Рассмотрим постановку оптимизационной задачи метода опорных векторов для линейно разделимой выборки:
    \begin{align*}
        \begin{cases}
            \frac{1}{2} \| w\|^2 \to \min_{w, b},\\
            y_i (\langle w, x\rangle + b) \ge 1, \quad i = \overline{1, \ell},
        \end{cases}
    \end{align*}
    а также её видоизменёный вариант для некоторого значения $t > 0$:
\begin{align*}
        \begin{cases}
            \frac{1}{2} \| w\|^2 \to \min_{w, b},\\
            y_i (\langle w, x\rangle + b) \ge t, \quad i = \overline{1, \ell}.
        \end{cases}
    \end{align*}
     Покажите, что разделяющие гиперплоскости, получающиеся в результате решения каждой из этих задач, совпадают.
\end{esProblem}


% \begin{esProblem}
%     Пусть мы решили двойственную задачу SVM и получили
%     оптимальные значения~$(\lambda_1, \dots, \lambda_{\ell})$,
%     где~$\lambda_5 = C/3$, $\lambda_2 = 0$.
%     Выразите оптимальное значение порога $b$ для~прямой задачи через
%     найденное решение~$(\lambda_1, \dots, \lambda_{\ell})$
%     двойственной задачи.
% \end{esProblem}


\begin{esProblem}
    Ответьте на следующие вопросы:
    \begin{enumerate}
    \item Почему в общем случае распределение $p(y|x)$ для некоторого объекта $x \in \mathbb{X}$ отличается от вырожденного ($p(y|x) \in \{0,1\}$)?
    \item Почему логистическая регрессия позволяет предсказывать корректные вероятности принадлежности объекта классам?
    \item Рассмотрим оптимизационную задачу из варианта SVM для линейно разделимых выборок. Всегда ли в обучающей выборке существует объект $x_i$, для которого выполнено $y_i (\langle w, x_i \rangle + b) = 1$? Почему?
    \item С какой целью в постановке оптимизационной задачи SVM для линейно неразделимых выборок вводятся переменные $\xi_i, \, i = \overline{1, \ell}?$
    \end{enumerate}
\end{esProblem}


\begin{esProblem}
    Рассмотрим целевую функцию логистической регрессии 
    \[
    Q(w) = \frac{1}{\ell} \sum_{i=1}^{\ell} \log (1 + \exp(-y_i \, \langle w, x_i\rangle)),
    \]
    
    \begin{enumerate} 
    \item Найдите градиент $\nabla Q_w$ и упростите итоговое выражение таким образом, чтобы в нём участвовала сигмоидная функция 
    $$\sigma(z) = \frac{1}{1 + \exp(-z)}.$$ При решении данной задачи вам может понадобиться следующий факт (убедитесь, что он действительно выполняется):
    $$\sigma'(z) = \sigma(z) (1- \sigma(z)).$$ 
    \item Выпишите, как будет выглядеть шаг градиентного спуска.
    \item Найдите вторую производную целевой функции по $w$.
    \item Выпишите квадратичную аппроксимацию для $Q(w)$ в окрестности $w=0$. Для этого разложите функцию потерь в ряд Тейлора до второго члена в окрестности точки $w=0$. С какой задачей совпадает задача минимизации квадратичной аппроксимации?
    \end{enumerate} 
\end{esProblem}

\end{document}




