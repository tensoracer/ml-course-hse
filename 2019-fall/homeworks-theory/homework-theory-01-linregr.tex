\documentclass[12pt,fleqn]{article}

\usepackage{../vkCourseML}

\theorembodyfont{\rmfamily}
\newtheorem{esProblem}{Задача}

\begin{document}

\title{Машинное обучение\\ФКН ВШЭ\\Теоретическое домашнее задание №1}

\date{}

\author{}

\maketitle


\begin{esProblem}
    Найдите производную по матрице $A \in \mathbb{R}^{n\times n}$

    \begin{equation*}
        \frac{\partial}{\partial A} \sum_{i=1}^n \lambda_i,
    \end{equation*}
    где $\lambda_1, \dots, \lambda_n$ --- набор собственных значений матрицы $A$.
\end{esProblem}

\begin{esProblem}
    Найдите производную по матрице $A \in \mathbb{R}^{n\times n}$

    \begin{equation*}
        \frac{\partial}{\partial A} \log \det A.
    \end{equation*}
\end{esProblem}

\begin{esProblem}

    Найдите производную по вектору $a \in \mathbb{R}^{n}$

    \begin{equation*}
        \frac{\partial}{\partial a} \left(
            a^T \exp(a a^T)a
        \right),
    \end{equation*}
    где $\exp(B)$~--- \href{https://en.wikipedia.org/wiki/Matrix_exponential}{матричная экспонента},
    $B \in \mathbb{R}^{n \times n}$.
    Матричной экспонентой обозначают ряд
    \begin{equation*}
        1 + \frac{B}{1!} + \frac{B^2}{2!} + \frac{B^3}{3!} + \frac{B^4}{4!} + \ldots = \sum_{k=0}^\infty \frac{B^k}{k!} .
    \end{equation*}
\end{esProblem}

\begin{esProblem}
    Рассмотрим задачу обучения линейной регрессии
    \begin{equation*}
        Q(w) = (y - Xw)^T(y - Xw) \rightarrow \min_{w}
    \end{equation*}
    Будем решать её с помощью градиентного спуска. Допустим, мы находимся на некоторой итерации $k$,
    и хотим выполнить очередной шаг
    \begin{equation*}
        w^{(k)} = w^{(k-1)} - \alpha \nabla_w Q(w^{(k - 1)}).
    \end{equation*}
    При известных $y$, $X$, $w^{(k-1)}$ найдите длину шага $\alpha$, при которой уменьшение значения функционала будет наибольшим:
    \[
        Q(w^{(k - 1)} - \alpha \nabla_w Q(w^{(k - 1)})) \to \min_{\alpha}.
    \]
\end{esProblem}

\begin{esProblem}
    Найдите константу C, решающую следующую задачу ($0 < \tau < 1$ фиксировано):
    \[
    \sum_{i=1}^\ell \rho_\tau (y_i - C) \rightarrow \min_{C},
    \]
    \[
    \rho_\tau(x) = \begin{cases} \tau x, \quad x > 0, \\ (\tau - 1) x, \quad x \leqslant 0. \end{cases}
    \]
\end{esProblem}

\begin{esProblem}
    Рассмотрим задачу регрессии для выборки~$X^\ell$,
    каждый объект которой описывается~$d$ признаками.
    Пусть все признаки принимают значения из интервала~$[0, 1]$.
    Разобьем гиперкуб~$[0, 1]^d$ равномерно на~$M = a^d$ областей,
    каждая из которых представляет собой подкуб со стороной~$1/a$.
    Пронумеруем все области.
    Обозначим через~$B_m$ множество индексов объектов обучающей выборки,
    попавших в области с номером~$m$,
    через~$n_m$~--- число таких объектов,
    а через~$r(x)$~--- номер области, в которую попадает объект~$x$.
    \emph{Гистограммным} называется алгоритм~$a(x)$, который возвращает для объекта~$x$
    средний ответ по всем обучающим объектам из области~$r(x)$:
    \[
        a(x)
        =
        \left\{
            \begin{aligned}
                &\frac{1}{n_{r(x)}}
                \sum_{i \in B_{r(x)}}
                    y_i,
                \quad
                &n_{r(x)} \neq 0;\\
                &0,
                \quad
                &n_{r(x)} = 0.
            \end{aligned}
        \right.
    \]

    Покажите, что в случае с гистограммным алгоритмом
    leave-one-out-оценка~(то есть оценка кросс-валидации с числом блоков~$k=\ell$) для квадратичной функции потерь
    может быть выписана аналитически.
    Ее вычисление по найденной вами формуле должно требовать
    не более~$O(\ell)$ операций.
\end{esProblem}

\begin{esProblem}
    Убедитесь, что вы знаете ответы на следующие вопросы:
    \begin{itemize}
        \item Почему~$L_1$-регуляризация производит отбор признаков?
        \item Почему коэффициент регуляризации нельзя подбирать по обучающей выборке?
        \item Почему накладывать регуляризатор на свободный коэффициент~$w_0$ может быть плохой идеей?
        \item Что такое кросс-валидация, чем она лучше использования отложенной выборки?
        \item Почему категориальные признаки нельзя закодировать натуральными числами? Что такое one-hot encoding?
        \item Для чего нужно масштабировать матрицу объекты-признаки перед обучением моделей машинного обучения?
        \item Почему MSE чувствительно к выбросам?
        %\item Что такое Huber Loss? В чем его преимущества по сравнению с MAE и MSE?
        %\item Почему квантильная регрессия так называется?
    \end{itemize}
\end{esProblem}


\end{document}

