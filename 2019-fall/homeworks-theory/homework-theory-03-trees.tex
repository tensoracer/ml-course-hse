\documentclass[12pt,fleqn]{article}
\usepackage{../lecture-notes/vkCourseML}


\theorembodyfont{\rmfamily}
\newtheorem{esProblem}{Задача}
\setcounter{section}{1}

\begin{document}
\title{Машинное обучение\\Теоретическое домашнее задание №3}
\date{}
\author{}
\maketitle

\begin{esProblem}
    На лекциях говорилось, что критерий информативности для набора объектов~$R$ вычисляется на основе того,
    насколько хорошо их целевые переменные предсказываются константой~(при оптимальном выборе этой константы):
    \[
        H(R)
        =
        \min_{c \in \YY}
        \frac{1}{|R|}
        \sum_{(x_i, y_i) \in R}
            L(y_i, c),
    \]
    где~$L(y, c)$~--- некоторая функция потерь.
    Соответственно, чтобы получить вид критерия при конкретной функции потерь, необходимо аналитически
    найти оптимальное значение константы и подставить его в формулу для~$H(R)$.

    Выведите критерии информативности для следующих функций потерь:
    \begin{enumerate}
        \item $L(y, c) = (y - c)^2$;
        \item $L(y, c) = \sum_{k = 1}^{K} (c_k - [y = k])^2$;
        \item $L(y, c) = -\sum_{k = 1}^{K} [y = k] \log c_k$.
    \end{enumerate}
    У вас должны получиться дисперсия, критерий Джини и энтропийный критерий соответственно.
\end{esProblem}

\begin{esProblem}
    Запишите оценку сложности построения одного решающего дерева в зависимости
    от размера обучающей выборки~$\ell$, числа признаков~$d$, максимальной глубины дерева~$D$.
    В качестве предикатов используются пороговые функции~$[x_j > t]$.
    При выборе предиката в каждой вершине перебираются все признаки,
    а в качестве порогов рассматриваются величины~$t$, равные значениям данного признака
    на объектах, попавших в текущую вершину.
    Считайте сложность вычисления критерия информативности константной.
\end{esProblem}

\end{document}
