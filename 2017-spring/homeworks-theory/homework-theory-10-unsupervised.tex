\documentclass[12pt,fleqn]{article}
\usepackage{../../vkCourseML}

\theorembodyfont{\rmfamily}
\newtheorem{esProblem}{Задача}

\title{Машинное обучение, ФКН ВШЭ\\Теоретическое домашнее задание №10}
\author{}
\date{}

\begin{document}
\maketitle

\begin{esProblem}
	Для двух одномерных нормальных распределений $\mathcal{N}(x\cond \mu_1, \sigma_1), \; \mathcal{N}(x\cond \mu_2, \sigma_2)$ найдите дивергенцию Кульбака-Лейблера.
	\begin{equation*}
		\text{KL}(\mathcal{N}(x\cond \mu_1, \sigma_1)\Vert \; \mathcal{N}(x\cond \mu_2, \sigma_2)) - ?
	\end{equation*}
\end{esProblem}

\begin{esProblem}
	Рассмотрим метод восстановления плотности распределения с помощью гистограмм. Разобьем все пространство на непересекающиеся области $\delta_i$. Каждому $\delta_i$ ставится в соответствие вероятность $h_i$. По заданной выборке $\{x_i\}_{i=1}^\ell$, найдите оптимальные значения $h_i$ с помощью метода максимального правдоподобия.
\end{esProblem}

\begin{esProblem}
    Рассмотрим общую схему EM-алгоритма,
    выводимую через разложение
    \[
        \log p(X \cond \Theta)
        =
        \LL(q, \Theta)
        +
        \KL{q}{p}.
    \]
    На E-шаге ищется распределение~$q$,
    доставляющее максимум нижней оценке~$\LL(q, \Theta^\text{old})$
    при фиксированном~$\Theta^\text{old}$.

    Модифицируем E-шаг: будем теперь искать максимум не среди всех
    возможных распределений, а лишь среди вырожденных,
    то есть присваивающих единичную вероятность одной точке
    и нулевую вероятность всем остальным.
    Как будут выглядеть E- и M-шаги в этом случае?
\end{esProblem}

\begin{esProblem}
	Наблюдается выборка бинарных значений $y = (y_1,\ldots, y_n), \; y_i\in\{0,1\}$. Все элементы выборки генерируются независимо, но известно, что в некоторый момент $z$ меняется частота генерации единиц. Т.е., для всех $i < z$ $P(y_i=1) = \theta_1$, а для всех $i \geq z$ $P(y_i=1) = \theta_2$. Необходимо вывести формулы для ЕМ-алгоритма, где $z$–скрытая переменная, а $\theta_1, \theta_2$ – параметры распределений.
\end{esProblem}

\begin{esProblem}
	Аналогично рассмотренному на семинаре релаксированному методу минимизации функционала $\text{RatioCut}(A_1,\ldots,A_k)$, сформулируйте и решите релаксированную задачу для минимизации функционала при~$k = 2$:
	\[
	\text{Ncut}(A_1,\ldots,A_k) = \frac12 \sum_{i=1}^k\frac{W(A_i, \overline{A}_i)}{\text{vol}(A_i)}, \;\;\; \text{vol}(A) = \sum_{i\in A} d_i.
	\]
\end{esProblem}

\begin{esProblem}

Для заданной матрицы попарных расстояний проведите спектральную кластеризацию на два кластера.
Метку кластера определите по знакам координат второго собственного вектора.
Подробно опишите шаги, которые вы сделали для получения результата.
Собственные векторы можно не считать руками~--- код для создания исходной матрицы записан ниже.

\[
\begin{bmatrix}
0.0 & \sqrt{26} & \sqrt{17} & \sqrt{10} & \sqrt{2} & 4.0 & \sqrt{20} & \sqrt{2} & 2.0 & 1.0 \\
\sqrt{26} & 0.0 & 1.0 & 2.0 & 4.0 & \sqrt{2} & \sqrt{2} & 6.0 & \sqrt{26} & 5.0 \\
\sqrt{17} & 1.0 & 0.0 & 1.0 & 3.0 & 1.0 & 1.0 & 5.0 & \sqrt{17} & 4.0 \\
\sqrt{10} & 2.0 & 1.0 & 0.0 & 2.0 & \sqrt{2} & \sqrt{2} & 4.0 & \sqrt{10} & 3.0 \\
\sqrt{2} & 4.0 & 3.0 & 2.0 & 0.0 & \sqrt{10} & \sqrt{10} & 2.0 & \sqrt{2} & 1.0 \\
4.0 & \sqrt{2} & 1.0 & \sqrt{2} & \sqrt{10} & 0.0 & 2.0 & \sqrt{26} & \sqrt{20} & \sqrt{17} \\
\sqrt{20} & \sqrt{2} & 1.0 & \sqrt{2} & \sqrt{10} & 2.0 & 0.0 & \sqrt{26} & 4.0 & \sqrt{17} \\
\sqrt{2} & 6.0 & 5.0 & 4.0 & 2.0 & \sqrt{26} & \sqrt{26} & 0.0 & \sqrt{2} & 1.0 \\
2.0 & \sqrt{26} & \sqrt{17} & \sqrt{10} & \sqrt{2} & \sqrt{20} & 4.0 & \sqrt{2} & 0.0 & 1.0 \\
1.0 & 5.0 & 4.0 & 3.0 & 1.0 & \sqrt{17} & \sqrt{17} & 1.0 & 1.0 & 0.0
\end{bmatrix}
\]

\begin{verbatim}
np.array([[0.0, np.sqrt(26.0), np.sqrt(17.0), np.sqrt(10.0), np.sqrt(2.0), \
4.0, np.sqrt(20.0), np.sqrt(2.0), 2.0, 1.0],
[np.sqrt(26.0), 0.0, 1.0, 2.0, 4.0, \
np.sqrt(2.0), np.sqrt(2.0), 6.0, np.sqrt(26.0), 5.0],
[np.sqrt(17.0), 1.0, 0.0, 1.0, 3.0, \
1.0, 1.0, 5.0, np.sqrt(17.0), 4.0],
[np.sqrt(10.0), 2.0, 1.0, 0.0, 2.0, \
np.sqrt(2.0), np.sqrt(2.0), 4.0, np.sqrt(10.0), 3.0],
[np.sqrt(2.0), 4.0, 3.0, 2.0, 0.0, \
np.sqrt(10.0), np.sqrt(10.0), 2.0, np.sqrt(2.0), 1.0],
[4.0, np.sqrt(2.0), 1.0, np.sqrt(2.0), np.sqrt(10.0), \
0.0, 2.0, np.sqrt(26.0), np.sqrt(20.0), np.sqrt(17.0)],
[np.sqrt(20.0), np.sqrt(2.0), 1.0, np.sqrt(2.0), np.sqrt(10.0), \
2.0, 0.0, np.sqrt(26.0), 4.0, np.sqrt(17.0)],
[np.sqrt(2.0), 6.0, 5.0, 4.0, 2.0, \
np.sqrt(26.0), np.sqrt(26.0), 0.0, np.sqrt(2.0), 1.0],
[2.0, np.sqrt(26.0), np.sqrt(17.0), np.sqrt(10.0), np.sqrt(2.0), \
np.sqrt(20.0), 4.0, np.sqrt(2.0), 0.0, 1.0],
[1.0, 5.0, 4.0, 3.0, 1.0, \
np.sqrt(17.0), np.sqrt(17.0), 1.0, 1.0, 0.0],])
\end{verbatim}

\end{esProblem}

\end{document} 
