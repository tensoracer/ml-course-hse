\documentclass[12pt,fleqn]{article}

\usepackage{../lecture-notes/vkCourseML}

\usepackage{lipsum}
\usepackage{indentfirst}
\title{Машинное обучение, ФКН ВШЭ \\Теоретическое домашнее задание №1 \\Линейные модели}
\author{}
\date{}
\theorembodyfont{\rmfamily}
\newtheorem{esProblem}{Задача}
\begin{document}

\maketitle

\begin{esProblem}
Скоро первая самостоятельная работа. Чтобы подготовиться к ней, ФКН ест конфеты и решает задачи. Число решённых задач $y$ зависит от числа съеденных конфет $x$. Если студент не съел ни одной конфеты, то он не хочет решать задачи. Поэтому для описания зависимости числа решённых задач от числа съеденных конфет используется линейная модель с одним признаком без константы $y_i = w \cdot x_i.$ В аналитическом виде найдите оценки параметра $w$, минимизируя следующие функции потерь:

\begin{enumerate}
    \item Линейная регрессия без штрафа: $Q(w) = \frac{1}{\ell} \sum_{i=1}^{\ell} (y_i - w x_i)^2$;
    \item Ridge-регрессия: $Q(w) = \frac{1}{\ell} \sum_{i=1}^{\ell} (y_i - w x_i)^2 + \lambda w^2$;
    \item LASSO-регрессия: $Q(w) = \frac{1}{\ell} \sum_{i=1}^{\ell} (y_i - w x_i)^2 + \lambda |w|$;
    \item Пусть решения этих задач равны $\hat{w}, \hat{w}_R$ и $\hat{w}_L$ соответственно. Найдите пределы 
        \begin{equation*} 
            \lim_{\lambda \to 0}  \hat{w}_R, \quad \lim_{\lambda \to \infty}  \hat{w}_R, \quad \lim_{\lambda \to 0}  \hat{w}_L, \quad \lim_{\lambda \to \infty}  \hat{w}_L.
        \end{equation*} 
    \item Как можно проинтерпретировать гиперпараметр $\lambda$? 
\end{enumerate}

\textbf{Hint:} в случае Lasso-регрессии придётся повозиться с модулем. Обратите внимание на то, что $Q(w)$ парабола, это поможет корректно найти аналитическое решение. Подумайте, с чем возникнут проблемы, если у нас будет не один параметр, а сотня. 
\end{esProblem}

\begin{esProblem}
Вася измерил вес трёх покемонов,  $y_1=6$, $y_2=6$, $y_3=10$.  Вася хочет спрогнозировать вес следующего покемона с помощью константной модели $y_i = w$. Для оценки параметра $w$ Вася использует целевую функцию

\begin{equation*} 
    \frac{1}{\ell}\sum_{i=1}^{\ell} (y_i - w)^2 + \lambda w^2.
\end{equation*} 

\begin{enumerate}
    \item Найдите оптимальное $w$ при произвольном $\lambda$. 
    \item Подберите оптимальное $\lambda$ с помощью кросс-валидации leave one out («выкинь одного»). На первом шаге мы оцениваем модель на всей выборке без первого наблюдения, а на первом тестируем её. На втором шаге мы оцениваем модель на всей выборке без второго наблюдения, а на втором тестируем её. И так далее $\ell$ раз. Чтобы найти $\lambda_{CV}$ мы минимизируем среднюю ошибку, допущенную на тестовых выборках.
	\item Найдите оптимальное значение $w$ при $\lambda_{CV}$, подобранном на предыдущем шаге. 
\end{enumerate}
\end{esProblem}

\begin{esProblem}
    Убедитесь, что вы знаете ответы на следующие вопросы:
    \begin{itemize}
        \item Что такое гиперпараметр модели и чем он отличается от параметра модели?
        \item Почему коэффициент регуляризации нельзя подбирать по обучающей выборке? Как подобрать оптимальное значение для коэффициента регуляризации? 
        \item Почему накладывать регуляризатор на свободный коэффициент~$w_0$ может быть плохой идеей?
        \item Что такое кросс-валидация, чем она лучше использования отложенной выборки?
        \item Почему категориальные признаки нельзя закодировать натуральными числами? Что такое one-hot encoding?
        \item Для чего нужно масштабировать матрицу объекты-признаки перед обучением моделей машинного обучения?
        \item Почему~$L_1$-регуляризация производит отбор признаков?
        \item Почему MSE чувствительно к выбросам? 
    \end{itemize}
\end{esProblem}

\end{document}
