\documentclass[12pt,fleqn]{article}
\usepackage{vkCourseML}
\hypersetup{unicode=true}
%\usepackage[a4paper]{geometry}
\usepackage[hyphenbreaks]{breakurl}

\interfootnotelinepenalty=10000

\begin{document}
\title{Лекция 4\\Линейная регрессия}
\author{Е.\,А.\,Соколов\\ФКН ВШЭ}
\maketitle

\section{Регуляризация}

Раньше мы обсуждали, что если матрица~$X^T X$ не является обратимой,
то с оптимизацией среднеквадратичной ошибки могут возникнуть некоторые трудности.
Действительно, в ряде случаев (признаков больше чем объектов, коррелирующие признаки)
оптимизационная задача~$Q(w) \to \min$ может иметь бесконечное число решений,
большинство которых являются переобученными и плохо работают на тестовых данных.
Покажем это.

Пусть в выборке есть линейно зависимые признаки.
Это по определению означает, что существует такой вектор~$v$,
что для любого объекта~$x$ выполнено~$\langle v, x \rangle = 0$.
Допустим, мы нашли оптимальный вектор весов~$w$ для линейного классификатора.
Но тогда классификаторы с векторами~$w + \alpha v$ будут давать~\emph{точно такие же}
ответы на всех объектах, поскольку
\[
    \langle w + \alpha v, x \rangle
    =
    \langle w, x \rangle
    +
    \alpha
    \underbrace{
        \langle v, x \rangle
    }_{=0}
    =
    \langle w, x \rangle.
\]
Это значит, что метод оптимизации может найти решение
со сколько угодно большими весами.
Такие решения не очень хороши, поскольку классификатор
будет чувствителен к крайне маленьким изменениям
в признаках объекта, а значит, переобучен.

Мы уже знаем, что переобучение нередко приводит к большим значениям коэффициентов.
Чтобы решить проблему, добавим к функционалу~\emph{регуляризатор},
который штрафует за слишком большую норму вектора весов:
\[
    Q_\alpha(w) = Q(w) + \alpha R(w).
\]

Наиболее распространенными являются~$L_2$ и~$L_1$-регуляризаторы:
\begin{align*}
    &R(w) = \|w\|_2 = \sum_{i = 1}^d w_i^2, \\
    &R(w) = \|w\|_1 = \sum_{i = 1}^d |w_i|.
\end{align*}

Коэффициент~$\alpha$ называется параметром регуляризации и контролирует баланс между
подгонкой под обучающую выборку и штрафом за излишнюю сложность.
Разумеется, значение данного параметра следует подбирать под каждую задачу.

Отметим, что свободный коэффициент~$w_0$ нет смысла регуляризовывать~---
если мы будем штрафовать за его величину, то получится, что
мы учитываем некие априорные представления о близости целевой переменной к нулю
и отсутствии необходимости в учёте её смещения.
Такое предположение является достаточно странным.
Особенно об этом следует помнить, если в выборке есть константный признак
и коэффициент~$w_0$ обучается наряду с остальными весами;
в этом случае следует исключить слагаемое, соответствующее константному признаку, из регуляризатора.

Квадратичный~(или~$L_2$) регуляризатор достаточно прост в использовании
в отличие от~$L_1$-регуляризатора, у которого нет производной в нуле.
При этом~$L_1$-регуляризатор имеет интересную особенность:
его использование приводит к занулению части весов.
Позже мы подробно обсудим это явление.

Обратим внимание на вид решения при использовании~$L_2$-регуляризации вместе со среднеквадратичной ошибкой.
В этом случае формулу для оптимального вектора весов можно записать в явном виде:
\[
    w
    =
    (X^T X + \alpha I)^{-1} X^T y.
\]
Благодаря добавлению диагональной матрицы к~$X^T X$ данная матрица
оказывается положительно определённой, и поэтому её можно обратить.
Таким образом, при использовании~$L_2$ регуляризации решение всегда будет единственным.

\section{Гиперпараметры}

В машинном обучении принято разделять подлежащие настройке величины
на~\emph{параметры} и~\emph{гиперпараметры}.
Параметрами называют величины, которые настраиваются по обучающей выборке~--- например,
веса в линейной регрессии.
К гиперпараметрам относят величины, которые контролируют сам процесс обучения и
не могут быть подобраны по обучающей выборке.

Хорошим примером гиперпараметра является коэффициент регуляризации~$\alpha$.
Введение регуляризации мешает модели подгоняться под обучающие данные,
и с точки зрения среднеквадратичной ошибки выгодно всегда брать~$\alpha = 0$.
Разумеется, такой выбор не будет оптимальным с точки зрения качества на новых данных,
и поэтому коэффициент регуляризации~(как и другие гиперпараметры) следует
настраивать по отложенной выборке или с помощью кросс-валидации.

При подборе гиперпараметров по кросс-валидации возникает проблема:
мы используем отложенные данные, чтобы выбрать лучший набор гиперпараметров.
По сути, отложенная выборка тоже становится обучающей, и показатели качества на ней
перестают характеризовать обобщающую способность модели.
В таких случаях выборку, на которой настраиваются гиперпараметры,
называют валидационной, и при этом выделяют третий, тестовый набор данных,
на которых оценивается качество итоговой модели.

\section{Разреженные модели}

В процессе обсуждения регуляризации мы упомянули, что использование~$L_1$-регуляризатора
приводит к обнулению части весов в модели.
Обсудим подробнее, зачем это может понадобиться и почему так происходит.

Модели, в которых некоторые веса равны нулю, называют~\emph{разреженными},
поскольку прогноз в них зависит лишь от части признаков.
Потребность в таких моделях можно возникнуть по многим причинам.
Несколько примеров:
\begin{enumerate}
    \item Может быть заведомо известно, что релевантными являются не все признаки.
        Очевидно, что признаки, которые не имеют отношения к задаче, надо исключать из данных,
        то есть производить~\emph{отбор признаков}.
        Есть много способов решения этой задачи, и~$L_1$-регуляризация~--- один из них.
    \item К модели могут выдвигаться ограничения по скорости построения предсказаний.
        В этом случае модель должна зависеть от небольшого количества наиболее важных признаков,
        и тут тоже оказывается полезной~$L_1$-регуляризация.
    \item В обучающей выборке объектов может быть существенно меньше, чем признаков~(так называемая~<<проблема $N \ll p$>>).
        Поскольку параметров линейной модели при этом тоже больше, чем объектов, задача обучения оказывается
        некорректной~--- решений много, и сложно выбрать из них то, которое обладает
        хорошей обобщающей способностью.
        Решить эту проблему можно путём внедрения в процесс обучения априорного знания о том,
        что целевая переменная зависит от небольшого количества признаков.
        Такая модификация как раз может быть сделана с помощью~$L_1$-регуляризатора.
\end{enumerate}
Теперь, когда мы представляем некоторые области применения разреженных моделей,
попробуем понять, почему~$L_1$ регуляризатор позволяет их обучать.
Этому есть несколько объяснений.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\linewidth]{pics/reg.eps}
    \caption{Линии уровня функционала качества, а также ограничения,
        задаваемые~$L_2$ и~$L_1$-регуляризаторами.}
    \label{pic:regularizers}
\end{figure}

\paragraph{Угловые точки.}
Можно показать, что если функционал~$Q(w)$ является выпуклым,
то задача безусловной минимизации функции~$Q(w) + \alpha \|w\|_1$
эквивалентна задаче условной оптимизации
\[
    \left\{
        \begin{aligned}
            & Q(w) \to \min_{w} \\
            & \|w\|_1 \leq C
        \end{aligned}
    \right.
\]
для некоторого~$C$.
На рис.~\ref{pic:regularizers}
изображены линии уровня функционала~$Q(w)$,
а также множество, определяемое ограничением~$\|w\|_1 \leq C$.
Решение определяется точкой пересечения допустимого множества с линией уровня,
ближайшей к безусловному минимуму.
Из изображения можно предположить, что в большинстве случаев эта точка будет лежать на одной из вершин ромба,
что соответствует решению с одной зануленной компонентой.

\paragraph{Штрафы при малых весах.}
Предположим, что текущий вектор весов состоит из двух элементов~$w = (1, \eps)$,
где~$\eps$ близко к нулю,
и мы хотим немного изменить данный вектор по одной из координат.
Найдём изменение~$L_2$- и~$L_1$-норм вектора при уменьшении первой компоненты на некоторое
положительное число~$\delta < \eps$:
\begin{align*}
    &\|w - (\delta, 0)\|_2^2
    =
    1 - 2 \delta + \delta^2 + \eps^2\\
    &\|w - (\delta, 0)\|_1
    =
    1 - \delta + \eps
\end{align*}
Вычислим то же самое для изменения второй компоненты:
\begin{align*}
    &\|w - (0, \delta)\|_2^2
    =
    1 - 2 \eps \delta + \delta^2 + \eps^2\\
    &\|w - (0, \delta)\|_1
    =
    1 - \delta + \eps
\end{align*}

Видно, что с точки зрения~$L_2$-нормы выгоднее уменьшать первую компоненту,
а для~$L_1$-нормы оба изменения равноценны.
Таким образом, при выборе~$L_2$-регуляризации гораздо меньше шансов,
что маленькие веса будут окончательно обнулены.

\paragraph{Проксимальный шаг.}
\emph{Проксимальные методы}~--- это класс методов оптимизации, которые
хорошо подходят для функционалов с негладкими слагаемыми.
Не будем сейчас останавливаться на принципах их работы, а приведём лишь
формулу для шага проксимального метода в применении к линейной регрессии
с квадратичным функционалом ошибки и~$L_1$-регуляризатором:
\[
    w^{(k)}
    =
    S_{\eta \alpha} \left(
        w^{(k - 1)}
        -
        \eta
        \nabla_w F(w^{(k - 1)})
    \right),
\]
где~$F(w) = \|Xw - y\|^2$~--- функционал ошибки без регуляризатора,
$\eta$~--- длина шага,
$\alpha$~--- коэффициент регуляризации,
а функция~$S_{\eta \alpha}(w)$ применяется к вектору весов покомпонентно,
и для одного элемента выглядит как
\[
    S_{\eta \alpha} (w_i)
    =
    \begin{cases}
        w_i - \eta \alpha, \quad &w_i > \eta \alpha\\
        0, \qquad &|w_i| < \eta \alpha\\
        w_i + \eta \alpha, \quad &w_i < -\eta \alpha
    \end{cases}
\]
Из формулы видно, что если на данном шаге значение некоторого веса не очень большое,
то на следующем шаге этот вес будет обнулён, причём чем больше коэффициент регуляризации,
тем больше весов будут обнуляться.

\section{Преобразования признаков}

\subsection{Нелинейные признаки}

С помощью линейной регрессии можно восстанавливать нелинейные зависимости,
если провести преобразование признакового пространства:
\[
    x = (x_1, \dots, x_d) \to \phi(x) = (\phi_1(x), \dots, \phi_m(x)).
\]
Например, можно перейти к квадратичным признакам:
\[
    \phi(x)
    =
    (x_1, \dots, x_d, x_1^2, \dots, x_d^2, x_1 x_2, \dots, x_{d-1} x_d).
\]
Линейная модель над новыми признаками уже сможет приближать любые квадратичные закономерности.
Аналогично можно работать и с полиномиальными признаками более высоких порядков.

Возможны и другие преобразования:
\begin{itemize}
    \item $\log x_j$~--- для признаков с тяжёлыми хвостами
    \item $\exp(\|x - \mu\|^2 / \sigma)$~--- для измерения близости до некоторой точки
    \item $\sin(x_j / T)$~--- для задач с периодическими зависимостями
\end{itemize}

\subsection{Масштабирование}

\begin{figure}[t]
    \centering
    \includegraphics[width=0.4\linewidth]{pics/descent.eps}
    \caption{Траектория градиентного спуска на функционале при признаках разного масштаба.}
    \label{pic:descent}
\end{figure}

При обучении линейных моделей полезно масштабировать признаки, то есть приводить их к единой шкале.
Разберёмся, зачем это нужно.

Рассмотрим функцию~$f_1(x) = \frac{1}{2} x_1^2 + \frac{1}{2} x_2^2$, выберем начальное приближение~$x^{(0)} = (1, 1)$
и запустим из него градиентный спуск с параметром $\eta = 1$.
Окажется, что за один шаг мы сможем сразу попасть в точку минимума.

Теперь~<<растянем>> функцию вдоль одной из осей:~$f_2(x) = 50 x_1^2 + \frac{1}{2} x_2^2$.
При таком же начальном приближении~$x^{(0)}$ антиградиент на первой итерации будет равен~$(-100, -1)$,
и попасть по нему в минимум уже невозможно~--- более того, при неаккуратном выборе длины шага можно
очень далеко уйти от минимума.
Пример траектории градиентного спуска при такой форме функции можно найти на рис.~\ref{pic:descent}.

Аналогичная проблема возникает с функционалом ошибки в линейной регрессии, если один из признаков
существенно отличается по масштабу от остальных.
Чтобы избежать этого, признаки следует масштабировать~--- например, путём стандартизации:
\[
    x_{ij}
    :=
    \frac{x_{ij} - \mu_j}{\sigma_j},
\]
где~$\mu_j = \frac{1}{\ell} \sum_{i = 1}^{\ell} x_{ij}$,
$\sigma_j = \frac{1}{\ell} \sum_{i = 1}^{\ell} (x_{ij} - \mu_j)^2$.
Или, например, можно масштабировать признаки на отрезок~$[0, 1]$:
\[
    x_{ij}
    :=
    \frac{x_{ij} - \min_i x_{ij}}{\max_i x_{ij} - \min_i x_{ij}}.
\]

\end{document}
