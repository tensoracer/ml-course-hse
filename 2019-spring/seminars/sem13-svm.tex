\documentclass[12pt,fleqn]{article}
\usepackage{../lecture-notes/vkCourseML}
\hypersetup{unicode=true}
%\usepackage[a4paper]{geometry}
\usepackage[hyphenbreaks]{breakurl}

\interfootnotelinepenalty=10000

\begin{document}
\title{Семинар 13\\Условная оптимизация}
\author{}
\maketitle

\section{Оптимизационные задачи и теорема Куна-Таккера}

Рассмотрим задачу минимизации
\begin{equation}
\label{eq:optProblem}
    \left\{
        \begin{aligned}
            & f_0(x) \to \min_{x \in \RR^d} \\
            & f_i(x) \leq 0, \quad i = 1, \dots, m, \\
            & h_i(x) = 0, \quad i = 1, \dots, p.
        \end{aligned}
    \right.
\end{equation}

Если ограничения в этой задаче отсутствуют,
то имеет место~\emph{необходимое условие экстремума}:
если в точке~$x$ функция~$f_0$ достигает своего минимума,
то ее градиент в этой точке равен нулю.
Значит, для решения задачи безусловной оптимизации
\[
    f_0(x) \to \min
\]
достаточно найти все решения уравнения
\[
    \nabla f_0(x) = 0,
\]
и выбрать то, в котором достигается наименьшее значение.
Для решения условных задач оптимизации требуется более сложный подход,
который мы сейчас и рассмотрим.

\subsection{Лагранжиан}
Задача условной оптимизации~\eqref{eq:optProblem} эквивалентна
следующей безусловной задаче:
\[
    f_0(x)
    +
    \sum_{i = 1}^{m} I_{-}(f_i(x))
    +
    \sum_{i = 1}^{p} I_{0}(h_i(x))
    \to
    \min_{x},
\]
где~$I_{-}(x)$~--- индикаторная функция для неположительных чисел:
\[
    I_{-}(x)
    =
    \left\{
        \begin{aligned}
            &0, \quad x \leq 0 \\
            &\infty, \quad x > 0,
        \end{aligned}
    \right.
\]
а~$I_{0}(x)$~--- индикаторная функция для нуля:
\[
    I_{0}(x)
    =
    \left\{
        \begin{aligned}
            &0, \quad x = 0 \\
            &\infty, \quad x \neq 0,
        \end{aligned}
    \right.
\]
Такая переформулировка, однако, не упрощает задачу~--- индикаторные
функции являются кусочно-постоянными и могут быть оптимизированы
лишь путем полного перебора решений.

Заменим теперь индикаторные функции на их линейные аппроксимации:
\[
    L(x, \lambda, \nu)
    =
    f_0(x)
    +
    \sum_{i = 1}^{m} \lambda_i f_i(x)
    +
    \sum_{i = 1}^{p} \nu_i h_i(x),
\]
где~$\lambda_i \geq 0$.
Полученная функция называется~\emph{лагранжианом} задачи~\eqref{eq:optProblem}.
Числа~$\lambda_i$ и~$\nu_i$ называются~\emph{множителями Лагранжа}
или~\emph{двойственными переменными}.

Конечно, линейные аппроксимации являются крайне грубыми,
однако их оказывается достаточно, чтобы получить необходимые условия
на решение исходной задачи.

\subsection{Двойственная функция}
\emph{Двойственной функцией} для задачи~\eqref{eq:optProblem} называется функция,
получающаяся при взятии минимума лагранжиана по~$x$:
\[
    g(\lambda, \nu)
    =
    \inf_{x} L(x, \lambda, \nu).
\]
Можно показать, что данная функция всегда является вогнутой.

Зачем нужна двойственная функция?
Оказывается, она дает нижнюю оценку на минимум в исходной оптимизационной задаче.
Обозначим решение задачи~\eqref{eq:optProblem} через~$x_*$.
Пусть~$x'$~--- \emph{допустимая} точка, т.е.~$f_i(x') \leq 0$, $h_i(x') = 0$.
Пусть также~$\lambda_i > 0$.
Тогда
\[
    L(x', \lambda, \nu)
    =
    f_0(x')
    +
    \sum_{i = 1}^{m} \lambda_i f_i(x')
    +
    \sum_{i = 1}^{p} \nu_i h_i(x')
    \leq
    f_0(x').
\]
Если взять в левой части минимум по всем допустимым~$x$, то неравенство останется верным;
оно останется верным и в случае, если мы возьмем минимум по всем возможным~$x$:
\[
    \inf_x L(x, \lambda, \nu)
    \leq
    \inf_{x \text{\ --- допуст.}} L(x, \lambda, \nu)
    \leq
    L(x', \lambda, \nu).
\]
Итак, получаем
\[
    \inf_{x} L(x, \lambda, \nu)
    \leq
    f_0(x').
\]
Поскольку решение задачи~$x_*$ также является допустимой точкой, получаем,
что при~$\lambda \geq 0$ двойственная функция дает нижнюю оценку на минимум:
\[
    g(\lambda, \nu) \leq f_0(x_*).
\]

\subsection{Двойственная задача}
Итак, двойственная функция для любой пары~$(\lambda, \nu)$ с~$\lambda > 0$
дает нижнюю оценку на минимум в оптимизационной задаче.
Попробуем теперь найти наилучшую нижнюю оценку:
\begin{equation}
\label{eq:dualProblem}
    \left\{
        \begin{aligned}
            & g(\lambda, \nu) \to \max_{\lambda, \nu} \\
            & \lambda_i \geq 0, \quad i = 1, \dots, m.
        \end{aligned}
    \right.
\end{equation}
Данная задача называется~\emph{двойственной} к задаче~\eqref{eq:optProblem}.
Заметим, что функционал в двойственной задаче всегда является вогнутым.

\subsection{Сильная и слабая двойственность}
Пусть~$(\lambda^*, \nu^*)$~--- решение двойственной задачи.
Значение двойственной функции всегда не превосходит условный
минимум исходной задачи:
\[
    g(\lambda^*, \nu^*)
    \leq
    f_0(x_*).
\]
Это свойство называется~\emph{слабой двойственностью}.
Разность~$f_0(x_*) - g(\lambda^*, \nu^*)$ называется~\emph{зазором}
между решениями прямой и двойственной задач.

Если имеет место равенство
\[
    g(\lambda^*, \nu^*)
    =
    f_0(x_*),
\]
то говорят о~\emph{сильной двойственности}.
Существует много достаточных условий сильной двойственности.
Одним из таких условий для выпуклых задач является условие Слейтера.
\emph{Выпуклой} задачей оптимизации называется задача
\[
    \left\{
        \begin{aligned}
            & f_0(x) \to \min_{x \in \RR^d} \\
            & f_i(x) \leq 0, \quad i = 1, \dots, m, \\
            & Ax = b.
        \end{aligned}
    \right.
\]
где функции~$f_0, f_1, \dots, f_m$ являются выпуклыми.
Условие Слейтера требует, чтобы существовала такая допустимая точка~$x'$,
в которой ограничения-неравенства выполнены строго:
\[
    \left\{
        \begin{aligned}
            & f_i(x) < 0, \quad i = 1, \dots, m, \\
            & Ax = b.
        \end{aligned}
    \right.
\]

Условие Слейтера можно ослабить: достаточно, чтобы
ограничения-неравенства были строгими только в том случае, если
они не являются линейными~(т.е. не имеют вид~$Ax - b$).

\subsection{Условия Куна-Таккера}
Пусть~$x_*$ и~$(\lambda^*, \nu^*)$~--- решения прямой и двойственной задач.
Будем считать, что имеет место сильная двойственность.
Тогда:
\begin{align*}
	f_0(x_*)
	&= g(\lambda^*, \nu^*) \\
	&=
	\inf_x \left(
	f_0(x)
	+
	\sum_{i = 1}^{m} \lambda_i^* f_i(x)
	+
	\sum_{i = 1}^{p} \nu_i^* h_i(x)
	\right) \\
	&\leq
	f_0(x_*)
	+
	\sum_{i = 1}^{m} \lambda_i^* f_i(x_*)
	+
	\sum_{i = 1}^{p} \nu_i^* h_i(x_*) \\
	&\leq
	f_0(x_*)
\end{align*}
Получаем, что все неравенства в этой цепочке выполнены как равенства.
Отсюда можно сделать несколько выводов.

Во-первых, если подставить в лагранжиан решение двойственной задачи~$(\lambda^*, \nu^*)$,
то его минимум будет достигаться на решении прямой задачи~$x_*$.
Иными словами, решение исходной задачи~\eqref{eq:optProblem} эквивалентно
минимизации лагранжиана~$L(x, \lambda^*, \nu^*)$
с подставленным решением двойственной задачи.

Во-вторых, из последнего неравенства получаем, что
\[
\sum_{i = 1}^{m} \lambda_i^* f_i(x_*) = 0.
\]
Каждый член неположителен, поэтому
\[
\lambda_i^* f_i(x_*) = 0, \quad i = 1, \dots, m.
\]
Эти условия называются~\emph{условиями дополняющей нежесткости}.
Они говорят, что множитель Лагранжа при~$i$-м ограничении
может быть не равен нулю лишь в том случае, если
ограничение выполнено с равенством~(в этом случае говорят,
что оно является~\emph{активным}).

Итак, мы можем записать условия, которые выполнены
для решений прямой и двойственной задач~$x_*$ и~$(\lambda^*, \nu^*)$:
\begin{equation}
\label{eq:kkt}
    \left\{
        \begin{aligned}
            & \nabla f_0(x_*)
                +
                \sum_{i = 1}^{m} \lambda_i^* \nabla f_i(x_*)
                +
                \sum_{i = 1}^{p} \nu_i^* \nabla h_i(x_*) = 0 \\
            & f_i(x_*) \leq 0, \quad i = 1, \dots m \\
            & h_i(x_*) = 0, \quad i = 1, \dots p \\
            & \lambda_i^* \geq 0, \quad i = 1, \dots m \\
            & \lambda_i^* f_i(x_*) = 0, \quad i = 1, \dots m
        \end{aligned}
    \right.
    \tag{$\text{KKT}$}
\end{equation}
Данные условия называются~\emph{условиями Куна-Таккера}~(в зарубежной
литературе их принято называть условиями Каруша-Куна-Таккера)
и являются необходимыми условиями экстремума.
Их можно сформулировать несколько иначе:
\begin{vkTheorem}
    Пусть~$x_*$~--- решение задачи~\eqref{eq:optProblem}.
    Тогда найдутся такие векторы~$\lambda^*$ и~$\nu^*$,
    что выполнены условия~\eqref{eq:kkt}.
\end{vkTheorem}

Если задача~\eqref{eq:optProblem} является выпуклой и удовлетворяет условию Слейтера,
то условия Куна-Таккера становятся~\emph{необходимыми и достаточными}.

\subsection{Экономическая интерпретация двойственной задачи}
Предположим, что мы хотим открыть фирму.
В нее мы можем нанимать программистов и менеджеров~--- обозначим их количество через~$x_1$ и~$x_2$ соответственно.
При этом каждый программист будет приносить~$c_1$ рублей в месяц, а каждый менеджер~--- $c_2$ рублей.
Труд каждого сотрудника должен оплачиваться.
Наша фирма может платить в двух формах~--- акциями и картошкой, причем
в месяц каждому программисту нужно выдать~$a_{11}$ акций и~$a_{21}$ килограммов картошки;
для менеджеров эти числа обозначим через~$a_{12}$ и~$a_{22}$.
Разумеется, наши возможности ограничены: мы можем тратить не больше~$b_1$ акций и~$b_2$ килограммов
картошки в месяц.
Запишем формально все эти соотношения:
\[
    \left\{
    \begin{aligned}
        &c_1 x_1 + c_2 x_2 \to \max_{x_1, x_2}\\
        &a_{11} x_1 + a_{12} x_2 \leq b_1\\
        &a_{21} x_1 + a_{22} x_2 \leq b_2\\
        &x_1 \geq 0, x_2 \geq 0
    \end{aligned}
    \right.
\]

Это задача линейного программирования, для которой легко найти двойственную:
\[
    \left\{
    \begin{aligned}
        &b_1 y_1 + b_2 y_2 \to \min_{y_1, y_2}\\
        &a_{11} y_1 + a_{21} y_2 \geq c_1\\
        &a_{12} y_1 + a_{22} y_2 \geq c_2\\
        &y_1 \geq 0, y_2 \geq 0
    \end{aligned}
    \right.
\]
Двойственную задачу можно проинтерпретировать следующим образом.
Допустим, что у нас появились другие дела, и вместо открытия фирмы
мы решили продать все ресурсы~(т.е. акции и картошку).
Разумеется, наши покупатели будут стремиться установить максимально низкую цену~---
иными словами, они будут минимизировать общую сумму сделки~$b_1 y_1 + b_2 y_2$,
где через~$y_1$ и~$y_2$ обозначены цены на одну акцию и на один килограмм картошки соответственно.
При этом у нас есть ограничение: мы не хотим продавать ресурсы дешевле, чем могли бы на них заработать,
если бы все же открыли фирму.
Это означает, что суммарная стоимость~$a_{11}$ акций и~$a_{21}$ килограммов
картошки~(т.е. размер оплаты одного программиста) не должна быть меньше, чем
доход от одного программиста~$c_1$.
Это требование, вкупе с аналогичным требованием к размеру оплаты менеджера,
как раз соответствует ограничениям в двойственной задаче.

Поскольку для данных задач имеет место сильная двойственность,
их решения будут совпадать.
Это означает, что оптимальная прибыль, которую можно получить при открытии фирмы,
совпадает с оптимальной выгодой от продажи всех ресурсов.


\begin{thebibliography}{1}
\bibitem{boyd04convex}
    \emph{Boyd, S., Vandenberghe, L.}
    Convex Optimization.~// Cambridge University Press, 2004.
\end{thebibliography}

\end{document}
