{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Машинное обучение, ФКН ВШЭ\n",
    "\n",
    "# Семинар 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:22.323388Z",
     "start_time": "2018-10-09T04:29:21.612396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Работа с текстовыми данными\n",
    "\n",
    "Разреженные матрицы имеют место в машинном обучении, в частности, в задачах обработки текстов. \n",
    "\n",
    "Как правило, модели машинного обучения действуют в предположении, что матрица \"объект-признак\" является вещественнозначной, поэтому при работе с текстами сперва для каждого из них необходимо составить его признаковое описание. Для этого широко используются техники векторизации, tf-idf и пр. Рассмотрим их на примере [датасета](https://www.dropbox.com/s/f9xsff8xluriy95/banki_responses.json.bz2?dl=0) отзывов о банках.\n",
    "\n",
    "Сперва загрузим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:24.769618Z",
     "start_time": "2018-10-09T04:29:22.637708Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:25.781553Z",
     "start_time": "2018-10-09T04:29:24.787533Z"
    }
   },
   "outputs": [],
   "source": [
    "data = fetch_20newsgroups(subset='all', categories=['comp.graphics', 'sci.med'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные содержат тексты новостей, которые надо классифицировать на разделы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:25.814348Z",
     "start_time": "2018-10-09T04:29:25.790992Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['comp.graphics', 'sci.med']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:25.880911Z",
     "start_time": "2018-10-09T04:29:25.853163Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = data['data']\n",
    "target = data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:25.905803Z",
     "start_time": "2018-10-09T04:29:25.892053Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: dyer@spdcc.com (Steve Dyer)\\nSubject: Re: Analgesics with Diuretics\\nOrganization: S.P. Dyer Computer Consulting, Cambridge MA\\n\\nIn article <ofk=lve00WB2AvUktO@andrew.cmu.edu> Lawrence Curcio <lc2b+@andrew.cmu.edu> writes:\\n>I sometimes see OTC preparations for muscle aches/back aches that\\n>combine aspirin with a diuretic.\\n\\nYou certainly do not see OTC preparations advertised as such.\\nThe only such ridiculous concoctions are nostrums for premenstrual\\nsyndrome, ostensibly to treat headache and \"bloating\" simultaneously.\\nThey\\'re worthless.\\n\\n>The idea seems to be to reduce\\n>inflammation by getting rid of fluid. Does this actually work? \\n\\nThat\\'s not the idea, and no, they don\\'t work.\\n\\n-- \\nSteve Dyer\\ndyer@ursa-major.spdcc.com aka {ima,harvard,rayssd,linus,m2c}!spdcc!dyer\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:25.960019Z",
     "start_time": "2018-10-09T04:29:25.944857Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sci.med'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target_names'][target[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Векторизация\n",
    "\n",
    "Самый очевидный способ формирования признакового описания текстов — векторизация. Пусть у нас имеется коллекция текстов $D = \\{d_i\\}_{i=1}^l$ и словарь всех слов, встречающихся в выборке $V = \\{v_j\\}_{j=1}^d.$ В этом случае некоторый текст $d_i$ описывается вектором $(x_{ij})_{j=1}^d,$ где\n",
    "$$x_{ij} = \\sum_{v \\in d_i} [v = v_j].$$\n",
    "\n",
    "Таким образом, текст $d_i$ описывается вектором количества вхождений каждого слова из словаря в данный текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:25.998948Z",
     "start_time": "2018-10-09T04:29:25.993421Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:28.231819Z",
     "start_time": "2018-10-09T04:29:26.007324Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(encoding='utf8', min_df=5)\n",
    "_ = vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результатом является разреженная матрица."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:28.257693Z",
     "start_time": "2018-10-09T04:29:28.245514Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x7889 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 75 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(texts[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:28.314870Z",
     "start_time": "2018-10-09T04:29:28.261625Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 75]\n",
      "[ 563  613  662  743  753  759  869  894  902  918 1025 1076 1334 1367\n",
      " 1477 1643 1686 1689 1769 1842 2024 2367 2378 2388 2472 2519 3005 3036\n",
      " 3111 3214 3401 3416 3595 3627 3677 3750 4138 4145 4231 4332 4334 4368\n",
      " 4752 4885 4912 5016 5057 5106 5128 5817 5825 5892 6109 6112 6337 6344\n",
      " 6502 6593 6627 6746 6818 6841 6951 7064 7066 7087 7096 7155 7234 7435\n",
      " 7759 7781 7798 7809 7856]\n",
      "[1 1 1 1 2 2 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 1 1 1 6 2 1 2 1 1 1 1 2 1 1 1 1\n",
      " 1 1 1 1 1 1 1 2 1 1 1 2 1 2 1 1 1 2 1 1 1 3 2 1 2 1 2 3 2 1 3 1 1 2 2 1 1\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.transform(texts[:1]).indptr)\n",
    "print(vectorizer.transform(texts[:1]).indices)\n",
    "print(vectorizer.transform(texts[:1]).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "Ещё один способ работы с текстовыми данными — [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf) (**T**erm **F**requency–**I**nverse **D**ocument **F**requency). Рассмотрим коллекцию текстов $D$.  Для каждого уникального слова $t$ из документа $d \\in D$ вычислим следующие величины:\n",
    "\n",
    "1. Term Frequency – количество вхождений слова в отношении к общему числу слов в тексте:\n",
    "$$\\text{tf}(t, d) = \\frac{n_{td}}{\\sum_{t \\in d} n_{td}},$$\n",
    "где $n_{td}$ — количество вхождений слова $t$ в текст $d$.\n",
    "1. Inverse Document Frequency\n",
    "$$\\text{idf}(t, D) = \\log \\frac{\\left| D \\right|}{\\left| \\{d\\in D: t \\in d\\} \\right|},$$\n",
    "где $\\left| \\{d\\in D: t \\in d\\} \\right|$ – количество текстов в коллекции, содержащих слово $t$.\n",
    "\n",
    "Тогда для каждой пары (слово, текст) $(t, d)$ вычислим величину:\n",
    "$$\\text{tf-idf}(t,d, D) = \\text{tf}(t, d)\\cdot \\text{idf}(t, D).$$\n",
    "\n",
    "Отметим, что значение $\\text{tf}(t, d)$ корректируется для часто встречающихся общеупотребимых слов при помощи значения $\\text{idf}(t, D).$\n",
    "\n",
    "Признаковым описанием одного объекта $d \\in D$ будет вектор $\\bigg(\\text{tf-idf}(t,d, D)\\bigg)_{t\\in V}$, где $V$ – словарь всех слов, встречающихся в коллекции $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:28.362162Z",
     "start_time": "2018-10-09T04:29:28.333428Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:30.817515Z",
     "start_time": "2018-10-09T04:29:28.393224Z"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='utf8', min_df=5)\n",
    "_ = vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На выходе получаем разреженную матрицу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:30.887142Z",
     "start_time": "2018-10-09T04:29:30.821702Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x7889 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 75 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.transform(texts[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:30.934150Z",
     "start_time": "2018-10-09T04:29:30.891727Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 75]\n",
      "[7856 7809 7798 7781 7759 7435 7234 7155 7096 7087 7066 7064 6951 6841\n",
      " 6818 6746 6627 6593 6502 6344 6337 6112 6109 5892 5825 5817 5128 5106\n",
      " 5057 5016 4912 4885 4752 4368 4334 4332 4231 4145 4138 3750 3677 3627\n",
      " 3595 3416 3401 3214 3111 3036 3005 2519 2472 2388 2378 2367 2024 1842\n",
      " 1769 1689 1686 1643 1477 1367 1334 1076 1025  918  902  894  869  759\n",
      "  753  743  662  613  563]\n",
      "[0.03193224 0.03493043 0.1191548  0.11616465 0.06386448 0.10254433\n",
      " 0.08646835 0.0635189  0.02926056 0.08659203 0.06258286 0.05345822\n",
      " 0.08196547 0.10988396 0.01897323 0.14127187 0.28780747 0.0817001\n",
      " 0.11277083 0.06495833 0.11172894 0.11569557 0.10908706 0.09333267\n",
      " 0.05629497 0.10419521 0.21200711 0.0196018  0.0498408  0.02125995\n",
      " 0.06479382 0.04336577 0.09831057 0.07314846 0.0825076  0.10419521\n",
      " 0.09593582 0.12115382 0.09961959 0.10254433 0.02286235 0.10254433\n",
      " 0.15248369 0.1003096  0.0825076  0.07005655 0.01897323 0.04990768\n",
      " 0.11146181 0.05551957 0.55711388 0.04382553 0.04470476 0.03932164\n",
      " 0.1191548  0.10335182 0.04949981 0.1141769  0.07462829 0.18970269\n",
      " 0.07684422 0.09593582 0.03949181 0.03030208 0.06203359 0.12592207\n",
      " 0.03533799 0.03566114 0.03279303 0.17958507 0.04515974 0.12115382\n",
      " 0.0943315  0.12592207 0.06802901]\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.transform(texts[:1]).indptr)\n",
    "print(vectorizer.transform(texts[:1]).indices)\n",
    "print(vectorizer.transform(texts[:1]).data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметим, что оба метода возвращают вектор длины 35189 (размер нашего словаря)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Классификация\n",
    "\n",
    "Воспользуемся изученными методами обработки текстов для решения задачи классификации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:32.290228Z",
     "start_time": "2018-10-09T04:29:30.940425Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(encoding='utf8', min_df=5)\n",
    "vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:33.399425Z",
     "start_time": "2018-10-09T04:29:32.293618Z"
    }
   },
   "outputs": [],
   "source": [
    "X = vectorizer.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:33.753796Z",
     "start_time": "2018-10-09T04:29:33.408010Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ekayumov/anaconda/envs/py36/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score, ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:34.547388Z",
     "start_time": "2018-10-09T04:29:33.783704Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.985, ACC: 0.952\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(X.shape[0], n_iter=1, test_size=0.3)\n",
    "for train_ids, test_ids in cv:\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X[train_ids], target[train_ids])\n",
    "    preds = lr.predict_proba(X[test_ids])[:,1]\n",
    "    print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(target[test_ids], preds), \n",
    "                                        accuracy_score(target[test_ids], (preds > 0.5).astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То же самое с tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:38.534306Z",
     "start_time": "2018-10-09T04:29:34.559271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=5,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(encoding='utf8', min_df=5)\n",
    "vectorizer.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:41.259145Z",
     "start_time": "2018-10-09T04:29:38.702676Z"
    }
   },
   "outputs": [],
   "source": [
    "X = vectorizer.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:41.396161Z",
     "start_time": "2018-10-09T04:29:41.274717Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC: 0.995, ACC: 0.969\n"
     ]
    }
   ],
   "source": [
    "cv = ShuffleSplit(X.shape[0], n_iter=1, test_size=0.3)\n",
    "for train_ids, test_ids in cv:\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X[train_ids], target[train_ids])\n",
    "    preds = lr.predict_proba(X[test_ids])[:,1]\n",
    "    print('ROC-AUC: %.3f, ACC: %.3f' % (roc_auc_score(target[test_ids], preds), \n",
    "                                        accuracy_score(target[test_ids], (preds > 0.5).astype(int))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Важность признаков\n",
    "\n",
    "Как уже упоминалось ранее, веса признаков в линейной модели в случае, если признаки отмасштабированы, характеризуют степень их влияния на значение целевой переменной. В задаче классификации текстов, кроме того, признаки являются хорошо интерпретируемыми, поскольку каждый из них соответствует конкретному слову. Изучим влияние конкретных слов на значение целевой переменной:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:41.513675Z",
     "start_time": "2018-10-09T04:29:41.422343Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "msg, 1.70\n",
      "pitt, 1.68\n",
      "she, 1.66\n",
      "my, 1.65\n",
      "health, 1.41\n",
      "medical, 1.41\n",
      "doctor, 1.36\n",
      "com, 1.28\n",
      "disease, 1.27\n",
      "of, 1.19\n",
      "cancer, 1.16\n",
      "banks, 1.15\n",
      "is, 1.13\n",
      "geb, 1.12\n",
      "in, 1.09\n",
      "that, 1.06\n",
      "your, 1.06\n",
      "photography, 1.05\n",
      "water, 1.04\n",
      "gordon, 1.02\n",
      "he, 1.00\n",
      "pain, 1.00\n",
      "her, 0.99\n",
      "treatment, 0.97\n",
      "medicine, 0.96\n",
      "not, 0.95\n",
      "dyer, 0.95\n",
      "food, 0.95\n",
      "portal, 0.93\n",
      "...\n",
      "where, -0.98\n",
      "pov, -0.99\n",
      "need, -1.01\n",
      "card, -1.03\n",
      "pc, -1.03\n",
      "display, -1.03\n",
      "hacker, -1.06\n",
      "ac, -1.09\n",
      "comp, -1.09\n",
      "screen, -1.09\n",
      "color, -1.16\n",
      "windows, -1.20\n",
      "points, -1.21\n",
      "ftp, -1.23\n",
      "video, -1.24\n",
      "polygon, -1.27\n",
      "format, -1.27\n",
      "software, -1.29\n",
      "computer, -1.32\n",
      "bit, -1.35\n",
      "gif, -1.36\n",
      "tiff, -1.39\n",
      "program, -1.44\n",
      "code, -1.47\n",
      "file, -1.66\n",
      "images, -1.67\n",
      "files, -1.76\n",
      "3d, -1.93\n",
      "image, -2.17\n"
     ]
    }
   ],
   "source": [
    "f_weights = zip(vectorizer.get_feature_names(), lr.coef_[0])\n",
    "f_weights = sorted(f_weights, key=lambda i: i[1])\n",
    "for i in range(1,30):\n",
    "    print('%s, %.2f' % f_weights[-i])\n",
    "    \n",
    "print('...')\n",
    "for i in reversed(range(1,30)):\n",
    "    print('%s, %.2f' % f_weights[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Лемматизация и стемминг\n",
    "\n",
    "Заметим, что одно и то же слово может встречаться в различных формах (например, \"сотрудник\" и \"сотрудника\"), но описанные выше методы интерпретируют их как различные слова, что делает признаковое описание избыточным. Устранить эту проблему можно при помощи **лемматизации** и **стемминга**.\n",
    "\n",
    "### Стемминг\n",
    "\n",
    "[**Stemming**](https://en.wikipedia.org/wiki/Stemming) –  это процесс нахождения основы слова. В результате применения данной процедуры однокоренные слова, как правило, преобразуются к одинаковому виду.\n",
    "\n",
    "**Примеры стемминга:**\n",
    "\n",
    "| Word        | Stem           |\n",
    "| ----------- |:-------------:|\n",
    "| вагон | вагон |\n",
    "| вагона | вагон |\n",
    "| вагоне | вагон |\n",
    "| вагонов | вагон |\n",
    "| вагоном | вагон |\n",
    "| вагоны | вагон |\n",
    "| важная | важн |\n",
    "| важнее | важн |\n",
    "| важнейшие | важн |\n",
    "| важнейшими | важн |\n",
    "| важничал | важнича |\n",
    "| важно | важн |\n",
    "\n",
    "[Snowball](http://snowball.tartarus.org/) – фрэймворк для написания алгоритмов стемминга. Алгоритмы стемминга отличаются для разных языков и используют знания о конкретном языке – списки окончаний для разных чистей речи, разных склонений и т.д. Пример алгоритма для русского языка – [Russian stemming](http://snowballstem.org/algorithms/russian/stemmer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:42.991276Z",
     "start_time": "2018-10-09T04:29:41.526134Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:29:43.034763Z",
     "start_time": "2018-10-09T04:29:43.007406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "машин обучен\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.snowball.RussianStemmer()\n",
    "print(stemmer.stem(u'машинное'), stemmer.stem(u'обучение'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.219132Z",
     "start_time": "2018-10-09T04:29:43.039613Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:22<00:00, 45.29it/s]\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.snowball.EnglishStemmer()\n",
    "\n",
    "def stem_text(text, stemmer):\n",
    "    tokens = text.split()\n",
    "    return ' '.join(map(lambda w: stemmer.stem(w), tokens))\n",
    "\n",
    "stemmed_texts = []\n",
    "for t in tqdm(texts[:1000]):\n",
    "    stemmed_texts.append(stem_text(t, stemmer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.235785Z",
     "start_time": "2018-10-09T04:30:05.224774Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: dyer@spdcc.com (Steve Dyer)\n",
      "Subject: Re: Analgesics with Diuretics\n",
      "Organization: S.P. Dyer Computer Consulting, Cambridge MA\n",
      "\n",
      "In article <ofk=lve00WB2AvUktO@andrew.cmu.edu> Lawrence Curcio <lc2b+@andrew.cmu.edu> writes:\n",
      ">I sometimes see OTC preparations for muscle aches/back aches that\n",
      ">combine aspirin with a diuretic.\n",
      "\n",
      "You certainly do not see OTC preparations advertised as such.\n",
      "The only such ridiculous concoctions are nostrums for premenstrual\n",
      "syndrome, ostensibly to treat headache and \"bloating\" simultaneously.\n",
      "They're worthless.\n",
      "\n",
      ">The idea seems to be to reduce\n",
      ">inflammation by getting rid of fluid. Does this actually work? \n",
      "\n",
      "That's not the idea, and no, they don't work.\n",
      "\n",
      "-- \n",
      "Steve Dyer\n",
      "dyer@ursa-major.spdcc.com aka {ima,harvard,rayssd,linus,m2c}!spdcc!dyer\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.253061Z",
     "start_time": "2018-10-09T04:30:05.242491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from: dyer@spdcc.com (steve dyer) subject: re: analges with diuret organization: s.p. dyer comput consulting, cambridg ma in articl <ofk=lve00wb2avukto@andrew.cmu.edu> lawrenc curcio <lc2b+@andrew.cmu.edu> writes: >i sometim see otc prepar for muscl aches/back ach that >combin aspirin with a diuretic. you certain do not see otc prepar advertis as such. the onli such ridicul concoct are nostrum for premenstru syndrome, ostens to treat headach and \"bloating\" simultaneously. they'r worthless. >the idea seem to be to reduc >inflamm by get rid of fluid. doe this actual work? that not the idea, and no, they don't work. -- steve dyer dyer@ursa-major.spdcc.com aka {ima,harvard,rayssd,linus,m2c}!spdcc!dy\n"
     ]
    }
   ],
   "source": [
    "print(stemmed_texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видим, стеммер работает не очень быстро и запускать его для всей выборки достаточно накладно."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация\n",
    "\n",
    "[Лемматизация](https://en.wikipedia.org/wiki/Lemmatisation) — процесс приведения слова к его нормальной форме (**лемме**):\n",
    "- для существительных — именительный падеж, единственное число;\n",
    "- для прилагательных — именительный падеж, единственное число, мужской род;\n",
    "- для глаголов, причастий, деепричастий — глагол в инфинитиве."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, для русского языка есть библиотека pymorphy2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.379945Z",
     "start_time": "2018-10-09T04:30:05.258114Z"
    }
   },
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.396275Z",
     "start_time": "2018-10-09T04:30:05.382271Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='играющих', tag=OpencorporaTag('PRTF,impf,tran,pres,actv plur,gent'), normal_form='играть', score=0.16666666666666666, methods_stack=((<DictionaryAnalyzer>, 'играющих', 303, 34),))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "morph.parse('играющих')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним работу стеммера и лемматизатора на примере:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.421653Z",
     "start_time": "2018-10-09T04:30:05.405261Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "игра\n"
     ]
    }
   ],
   "source": [
    "stemmer = nltk.stem.snowball.RussianStemmer()\n",
    "print(stemmer.stem('играющих'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.439311Z",
     "start_time": "2018-10-09T04:30:05.426932Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "играть\n"
     ]
    }
   ],
   "source": [
    "print(morph.parse('играющих')[0].normal_form)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разреженные матрицы\n",
    "\n",
    "Разреженная матрица — это матрица, большинство элементов которой равны нулю. Такие матрицы возникают во многих областях науки, в том числе и в машинном обучении.\n",
    "\n",
    "Для разреженных матриц можно определить следующие характеристики:\n",
    "- разреженность (sparsity) — доля нулевых элементов матрицы,\n",
    "- плотность (density) — доля ненулевых элементов матрицы, или $1 - \\text(sparsity)$.\n",
    "\n",
    "Для разреженных матриц существуют специальные способы их хранения в памяти компьютера, при которых хранятся только ненулевые значения, тем самым сокращается объём занимаемой памяти. Эти способы реализованы в библиотеке [scipy.sparse](http://docs.scipy.org/doc/scipy/reference/sparse.html). Кроме того, разреженные матрицы поддерживаются большинством реализаций методов машинного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.459844Z",
     "start_time": "2018-10-09T04:30:05.444637Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### COOrdinate format\n",
    "\n",
    "[Координатный формат](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.coo_matrix.html#scipy.sparse.coo_matrix) задаёт матрицу при помощи троек (индекс строки, индекс столбца, значение элемента), описывающих ненулевые элементы матрицы. Как правило, тройки сортируют по индексу строки, а затем индексу столбца для ускорения работы. \n",
    "\n",
    "Объём занимаемой памяти — $O(n),$ где $n$ — число ненулевых элементов в матрице."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.760895Z",
     "start_time": "2018-10-09T04:30:05.751247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n"
     ]
    }
   ],
   "source": [
    "m = (np.arange(9) + 1).reshape(3,3)\n",
    "print(m)\n",
    "sparse_m = sp.coo_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.811107Z",
     "start_time": "2018-10-09T04:30:05.772652Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 0, 1)\n",
      "(0, 1, 2)\n",
      "(0, 2, 3)\n",
      "(1, 0, 4)\n",
      "(1, 1, 5)\n",
      "(1, 2, 6)\n",
      "(2, 0, 7)\n",
      "(2, 1, 8)\n",
      "(2, 2, 9)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sparse_m.data)):\n",
    "    print('(%d, %d, %d)' % (sparse_m.row[i], sparse_m.col[i], sparse_m.data[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для матрицы, содержащей нулевые элементы, имеем:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.853480Z",
     "start_time": "2018-10-09T04:30:05.815975Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 2. 0.]\n",
      " [0. 0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "m = np.eye(3)*np.arange(1,4)\n",
    "print(m)\n",
    "sparse_m = sp.coo_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.904891Z",
     "start_time": "2018-10-09T04:30:05.859255Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-39-272b5a3c2479>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-39-272b5a3c2479>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    print '(%d, %d, %d)' % (sparse_m.row[i], sparse_m.col[i], sparse_m.data[i])\u001b[0m\n\u001b[0m                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(sparse_m.data)):\n",
    "    print '(%d, %d, %d)' % (sparse_m.row[i], sparse_m.col[i], sparse_m.data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed Sparse Row matrix\n",
    "\n",
    "[CSR формат](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix) - разреженная по строчкам матрица. \n",
    "\n",
    "<img src=\"images/arrays.png\">\n",
    "\n",
    "Формат задаёт матрицу при помощи трёх массивов:\n",
    "1. $i$-ый элемент первого массива соответствует $i$-ой строке и содержит индекс некоторого элемента во втором массиве,\n",
    "2. во втором массиве по порядку для каждой строки записаны индексы столбцов ненулевых элементов,\n",
    "3. третий массив имеет такую же длину, как и второй, и содержит значения соответствующих ненулевых элементов.\n",
    "\n",
    "Обозначим описанные массивы $a,b,c$. Для получения элемента матрицы на позиции $(i, j)$ необходимо осуществить следующую последовательность действий:\n",
    "1. Получить значения $a[i]=k_{left}, a[i+1]=k_{right}$.\n",
    "2. Тогда индексы столбцов ненулевых элементов $i$-ой строки будут находиться в \"подмассиве\" $b[k_{left}:k_{right}]$.\n",
    "3. В цикле перебираем элементы подмассива $b[k_{left}:k_{right}]$, пока не встретим элемент, равный $j$.\n",
    "4. Если такой элемент обнаружен на позиции $m$ (в терминах массива $b$), то ответом является значение $c[m]$.\n",
    "5. Иначе ответом является 0.если мы не встретили элемент, равный $j$, то возвращаем $0$.\n",
    "\n",
    "Объём занимаемой памяти — $O(n)$, где $n$ - число ненулевых элементов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.907098Z",
     "start_time": "2018-10-09T04:29:21.695Z"
    }
   },
   "outputs": [],
   "source": [
    "m = (np.arange(9) + 1).reshape(3,3)\n",
    "print(m)\n",
    "sparse_m = sp.csr_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.930662Z",
     "start_time": "2018-10-09T04:29:21.701Z"
    }
   },
   "outputs": [],
   "source": [
    "print('a', sparse_m.indptr)\n",
    "print('b', sparse_m.indices)\n",
    "print('c', sparse_m.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для матрицы, содержащей нулевые элементы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.932336Z",
     "start_time": "2018-10-09T04:29:21.705Z"
    }
   },
   "outputs": [],
   "source": [
    "m = np.tril(np.arange(1,4))\n",
    "print(m)\n",
    "sparse_m = sp.csr_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.934620Z",
     "start_time": "2018-10-09T04:29:21.710Z"
    }
   },
   "outputs": [],
   "source": [
    "print('a', sparse_m.indptr)\n",
    "print('b', sparse_m.indices)\n",
    "print('c', sparse_m.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compressed Sparse Column matrix\n",
    "\n",
    "[CSC формат](http://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csc_matrix.html#scipy.sparse.csc_matrix) - разреженная по столбцам матрица. \n",
    "\n",
    "Формат CSC задаёт матрицу аналогично формату CSR, но при этом элементы первого массива соответствуют столбцам, а не строкам.\n",
    "\n",
    "Объём занимаемой памяти — $O(n)$, где $n$ - число ненулевых элементов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.937705Z",
     "start_time": "2018-10-09T04:29:21.716Z"
    }
   },
   "outputs": [],
   "source": [
    "m = (np.arange(9) + 1).reshape(3,3)\n",
    "print(m)\n",
    "sparse_m = sp.csc_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.943264Z",
     "start_time": "2018-10-09T04:29:21.721Z"
    }
   },
   "outputs": [],
   "source": [
    "print('a', sparse_m.indptr)\n",
    "print('b', sparse_m.indices)\n",
    "print('c', sparse_m.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.946962Z",
     "start_time": "2018-10-09T04:29:21.727Z"
    }
   },
   "outputs": [],
   "source": [
    "m = np.tril(np.arange(1,4))\n",
    "print(m)\n",
    "sparse_m = sp.csc_matrix(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-09T04:30:05.950375Z",
     "start_time": "2018-10-09T04:29:21.732Z"
    }
   },
   "outputs": [],
   "source": [
    "print('a', sparse_m.indptr)\n",
    "print('b', sparse_m.indices)\n",
    "print('c', sparse_m.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Умножение разреженных матриц\n",
    "\n",
    "Как мы убедились, объём занимаемой памяти практически не отличается для всех вариантов хранения разреженных матриц. В таком случае использование какого из вариантов даёт больше преимуществ? Оказывается, что все три способа кардинально различаются по времени умножения матриц.\n",
    "\n",
    "Для начала вспомним правило умножения матриц:\n",
    "$$C = A\\cdot B$$\n",
    "$$C_{ij} = \\sum_k A_{ik}B_{kj}$$\n",
    "\n",
    "Для нахождения элемента $C_{ij}$ необходимо получить $i$-ую строчку матрицы $A$ и $j$-ый столбец матрицы $B$. Исследуем время выполнения этих операций для каждого из форматов:\n",
    "\n",
    "- **COO.** Стоимость получения строки — $O(n)$. Стоимость получения столбца — $O(n)$. При условии, что тройки отсортированы, время поиска можно сократить, воспользовавшись бинарным поиском.\n",
    "- **CSR.** Стоимость получения строки — $O(1)$. Стоимость получения столбца — $O(n)$.\n",
    "- **CSC.** Стоимость получения строки — $O(n)$. Стоимость получения столбца — $O(1)$.\n",
    "\n",
    "Таким образом, время перемножения матриц будет оптимальным, если матрица $A$ задаётся в формате CSR, а матрица $B$ — в формате CSC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разреженные матрицы в линейных моделях\n",
    "\n",
    "Рассмотрим задачу линейной регрессии с функционалом качества MSE:\n",
    "\n",
    "$$Q = ||Xw - y||^2 \\rightarrow \\min_{w}.$$\n",
    "\n",
    "Как уже говорилось на предыдущих семинарах, вместо нахождения оптимального значения вектора $w$ используют градиентные методы оптимизации функционала. Запишем формулу его градиента:\n",
    "\n",
    "$$\\frac{\\partial Q}{\\partial w} = 2X^T(Xw - y).$$\n",
    "\n",
    "Заметим, что матрица $X$, заданная в формате CSR, может быть представлена как $X^T$ в формате CSC (действительно, используя те же массивы, мы можем придать им \"симметричный\" смысл).\n",
    "\n",
    "Рассмотрим, как осуществляется умножение разреженной матрицы $A$ на вектор $z$:\n",
    "\n",
    "1) **CSR**\n",
    "$$(Az)_{i} = \\sum_{k}A_{ik}z_k.$$\n",
    "\n",
    "Для матрицы в формале CSR обращение к строчкам матрицы выполняется за $O(1)$, поэтому перемножение выполняется за $O(n)$, где $n$ - кол-во ненулевых элементов матрицы $X$.\n",
    "    \n",
    "2) **CSC** \n",
    "\n",
    "Для матрицы в формате CSC обращение к строчкам матрицы выполняется за $O(n)$. В этом случае умножение будем производить следующим образом:\n",
    "    - Аллоцируем результирующий вектор, который предварительно заполним нулями. \n",
    "    - Обращаемся к $i$-ому столбцу матрицы $A$ и $i$-ому элементу вектора $z$.\n",
    "    - Каждый ненулевой элемент в столбце домножаем на $z_i$ и добавляем результат к соответствующему значению результирующего вектора.\n",
    "    \n",
    "Итого, для умножения разреженной матрицы на вектор получаем следующую асимптотику:\n",
    " - $O(l)$ по памяти;\n",
    " - $O(n)$ по времени.\n",
    "\n",
    "Таким образом, мы описали процедуру умножения разреженной матрицы на вектор, и теперь её можно применить для вычисления градиента в задачах с разреженными матрицами \"объект-признак\"."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
