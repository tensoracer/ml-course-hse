{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изначально повторяем все идеи из домашней работы: используем StandardScaler и PolynomialFeatures, обучаем ElasticNet на логарифме. Выкидываем столбец 'data', нам он пока не интересен. Самое первое, что очень сильно улучшает скор - это one hot encoding колонки 'zip-code'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "\n",
    "def mape(y_true, y_predict): \n",
    "    return np.mean(np.abs((y_true - y_predict) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\777\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\777\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.870633226133958"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load('x_train.npy')\n",
    "target = np.load('y_train.npy')\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "df.drop('date', axis=1, inplace=True)\n",
    "df = pd.get_dummies(data=df, columns=['zipcode'])\n",
    "\n",
    "Scaler = StandardScaler()\n",
    "df = Scaler.fit_transform(df)\n",
    "\n",
    "PolyFeatures = PolynomialFeatures(2)\n",
    "df = pd.DataFrame(PolyFeatures.fit_transform(df))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, train_size=0.8, random_state=42)\n",
    "\n",
    "Model = ElasticNet(alpha=0.00035)\n",
    "Model.fit(X_train, np.log(y_train))\n",
    "mape(y_test, np.exp(Model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь постепенно будем добавлять мелочи, которые немного улучшают скор. Поскольку мы хотим использовать PolynomialFeatures(), нам важно не сделать слишком много бесполезных столбцов. Третью степень после изменения 'zipcode' мы использовать не можем, уже слишком много. А вторая степень пока считается быстро, поэтому можно добавить какие-то признаки, которые не найдет PolynomialFeatures(2): логарифмы и экспоненты признаков, корни и прочее.\n",
    "Ещё можно добавлять признаки в другом направлении: например взять какую-нибудь точку и посчитать расстояния от всех других до неё. Посмотрим на 75% в колонках 'lat' и 'long' и посчитаем расстояния до неё."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(count    15000.000000\n",
       " mean        47.560677\n",
       " std          0.138608\n",
       " min         47.155899\n",
       " 25%         47.472900\n",
       " 50%         47.572050\n",
       " 75%         47.678625\n",
       " max         47.777599\n",
       " Name: lat, dtype: float64, count    15000.000000\n",
       " mean      -122.213005\n",
       " std          0.141256\n",
       " min       -122.518997\n",
       " 25%       -122.327003\n",
       " 50%       -122.228996\n",
       " 75%       -122.124001\n",
       " max       -121.315002\n",
       " Name: long, dtype: float64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load('x_train.npy')\n",
    "df = pd.DataFrame(X)\n",
    "df.lat.describe(), df.long.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё помогает умножать ответы на 0.99 и другие неадекватные вещи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\777\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\777\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.034993876247485"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.load('x_train.npy')\n",
    "target = np.load('y_train.npy')\n",
    "df = pd.DataFrame(X)\n",
    "\n",
    "df.drop('date', axis=1, inplace=True)\n",
    "df = pd.get_dummies(data=df, columns=['zipcode'])\n",
    "\n",
    "df['sqrt_living'] = np.sqrt(df.sqft_living)\n",
    "df['sqrt_lot'] = np.sqrt(df.sqft_lot)\n",
    "df['sqrt_above'] = np.sqrt(df.sqft_above)\n",
    "df['sqrt_basement'] = np.sqrt(df.sqft_basement)\n",
    "\n",
    "lat0 = 47.67\n",
    "long0 = -122.12\n",
    "df['dist'] = np.sqrt(np.square(df['lat'] - lat0) + np.square(df['long'] - long0))\n",
    "\n",
    "Scaler = StandardScaler()\n",
    "df = Scaler.fit_transform(df)\n",
    "\n",
    "PolyFeatures = PolynomialFeatures(2)\n",
    "df = pd.DataFrame(PolyFeatures.fit_transform(df))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, train_size=0.8, random_state=42)\n",
    "\n",
    "Model = ElasticNet(alpha=0.00035)\n",
    "Model.fit(X_train, np.log(y_train))\n",
    "mape(y_test, 0.99 * np.exp(Model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно продолжать добавлять сюда разные признаки, но сильно улучшить модель у меня больше не получалось. Но помогла другая идея - посмотреть на все признаки локально: по ближайшим N соседям (подбирая разное N, у меня лучший результат показало N=70). Сначала я просто посчитал среднюю цену и все средние площади вокруг каждого дома и сохранил это как новые данные, потому что считалось очень долго и делать это каждый раз было бы самоубийством. С новыми признаки делаем то же самое, что и раньше - извлекаем корни, берем отношения. Каждый признак отдельно проверяем на адекватность - если он улучшает скор, оставляем, если не улучшает - убираем. Так же можно заметить, что 'zipcode' нам больше не нужны и можно освободить кучу места, чтобы быстрее считалось и можно было больше экспериментивароть. Сейчас можно увеличить степень в PolynomialFeatures(), поскольку основную часть занимал 'zipcode', но это не дает прибавки к результату."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\777\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n",
      "C:\\Users\\777\\Anaconda3\\envs\\tensorflow-cpu\\lib\\site-packages\\sklearn\\linear_model\\coordinate_descent.py:491: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11.58994942305626"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.load('y_train.npy')\n",
    "df = pd.read_pickle('df70sq4')\n",
    "\n",
    "df.drop('date', axis=1, inplace=True)\n",
    "\n",
    "df['near_price_per_living'] = df['near_price'] / df['near_sqft_living']\n",
    "\n",
    "lat0 = 47.67\n",
    "long0 = -122.12\n",
    "df['dist'] = np.sqrt(np.square(df['lat'] - lat0) + np.square(df['long'] - long0))\n",
    "\n",
    "Scaler = StandardScaler()\n",
    "df = Scaler.fit_transform(df)\n",
    "\n",
    "PolyFeatures = PolynomialFeatures(2)\n",
    "df = pd.DataFrame(PolyFeatures.fit_transform(df))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, target, train_size=0.8, random_state=42)\n",
    "\n",
    "Model = ElasticNet(alpha=0.00035)\n",
    "Model.fit(X_train, np.log(y_train))\n",
    "mape(y_test, 0.99 * np.exp(Model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это очень забавно, но во время написания этого отчета я заметил, что все те многочисленные признаки совершенно не помогают (и даже мешают), поэтому в ячейке выше их нет. Я оставил только один, который действительно немного улучшает результат. Полезными оказались только посчитанные изначально - локальная средняя стоимость и локальные среднии площади."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
